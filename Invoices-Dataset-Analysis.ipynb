{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78cb3d7c",
   "metadata": {},
   "source": [
    "# Programming in Data Science - Final Project\n",
    "## Invoices Dataset Analysis\n",
    "**Team Members: Leo WINTER, Yoann SUBLET, Kellian VERVAELE KLEIN, Alvaro SERERO**\n",
    "\n",
    "Dataset: Invoices (Kaggle)\n",
    "\n",
    "Source: https://www.kaggle.com/datasets/cankatsrc/invoices/data\n",
    "\n",
    "This dataset includes multiple fields such as customer details (first name, last name, email), transaction information (product ID, quantity, amount, invoice date), and additional attributes like address, city, and stock code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cb51c4",
   "metadata": {},
   "source": [
    "### Import all needed libraries for the project:\n",
    "- Pandas for data manipulation\n",
    "- Plotly express for visualizations\n",
    "- Dash for creating a visual and interactive dashboard interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcdd6c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\leoma\\Documents\\anaconda3\\envs\\prophet_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from dash import Dash, dcc, html,Input, Output\n",
    "from dash import callback\n",
    "from prophet import Prophet\n",
    "from prophet.plot import plot_plotly\n",
    "import logging\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324203ca",
   "metadata": {},
   "source": [
    "## 1) Data collection and exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a26496a",
   "metadata": {},
   "source": [
    "### Function to safely load CSV data from a file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3179ec2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Function to load CSV data from a given file path safely.\n",
    "\n",
    "    Input: \n",
    "    ------\n",
    "    file_path => String, path to the CSV file\n",
    "\n",
    "    Output: \n",
    "    ------\n",
    "    dataset => pd.DataFrame containing the loaded data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(\"Data loaded successfully.\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d423915",
   "metadata": {},
   "source": [
    "### Function to process invalid first_name and last_name columns.\n",
    "- In the initial dataset there are \"last_name\" and \"first_name\" columns but each one contains a combination of a first name and last name which does not make sense since a trasaction is only made by one individual person and columns should contain exactly what is described by their name.\n",
    "- For example, the first line is structured as follows, which is a mistake and need to be corrected.\n",
    "\n",
    "first_name | last_name  \n",
    "Carmen Nixon | Todd Anderson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d601f854",
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_treatment(dataset: pd.DataFrame, options: str=\"separate\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Function to treat first_name and last_name columns in a dataset.\n",
    "\n",
    "    Only use this function if both first_name and last_name columns have already\n",
    "    the full name (first + last name)!!!\n",
    "    This function does not fuse firts_name and last_name column in a column name\n",
    "\n",
    "    \n",
    "    Input:\n",
    "    ---------\n",
    "    - dataset => Pandas DataFrame, dataset must have first_name and last_name columns\n",
    "    - options => String, options for treatment between \"separate\", \"first\" and \"last\"\n",
    "        - \"separate\" (default): create two new line for each name, \n",
    "        - \"first\": keep only the first_name renamed as name, \n",
    "        - \"last\" : keep only the last_name renamed as name\n",
    "    \n",
    "    Output:\n",
    "    ---------\n",
    "    - dataset => Pandas DataFrame after treating first_name and last_name columns\n",
    "    \"\"\"\n",
    "\n",
    "    if \"first_name\" in dataset and \"last_name\" in dataset:\n",
    "        # Create two new line by spearation of last_name and first_name column\n",
    "        # Create a new column named name\n",
    "        if options == \"separate\":\n",
    "            value = dataset.columns.difference(['first_name','last_name']).tolist()\n",
    "            new_dataset = pd.melt(dataset, id_vars=value,              \n",
    "                              value_vars=['first_name', 'last_name'],\n",
    "                              value_name='name')\n",
    "            \n",
    "            autres_colonnes = [col for col in new_dataset.columns if col not in [\"name\", \"variable\"]]\n",
    "            nouvel_ordre = [\"name\"] + autres_colonnes\n",
    "            new_dataset = new_dataset[nouvel_ordre]\n",
    "\n",
    "        elif options == \"first\":\n",
    "            # Delete the last_name column\n",
    "            # Create a new column named name\n",
    "            new_dataset = dataset.drop(columns=['last_name'])\n",
    "            new_dataset.rename(columns={'first_name': 'name'}, inplace=True)\n",
    "\n",
    "        elif options == \"last\":\n",
    "            # Delete the first_name column\n",
    "            # Create a new column named name\n",
    "            new_dataset = dataset.drop(columns=['first_name'])\n",
    "            new_dataset.rename(columns={'last_name': 'name'}, inplace=True)\n",
    "        else:\n",
    "            print(f\"{options} is not a correct parameters of options, please write 'separate' or 'first' or 'last\")\n",
    "            return dataset\n",
    "        return new_dataset\n",
    "    else:\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26aeba0c",
   "metadata": {},
   "source": [
    "### Function to parse invoice dates:\n",
    "- Convert \"invoice_date\" column to datetime for futural temporal manipulations.\n",
    "- Extracts year, month, day, and day of week features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bdbddd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_dates(df: pd.DataFrame,date_column: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Converts \"invoice_date\" column from string to datetime.\n",
    "    Extracts year, month, day, and day of week features.\n",
    "\n",
    "    Input:\n",
    "    ------\n",
    "    - df (DataFrame) - dataset with invoice_date column\n",
    "\n",
    "    Output:\n",
    "    ------\n",
    "    - df (DataFrame) - dataset with parsed datetime features\n",
    "    \"\"\"\n",
    "    if date_column not in df.columns:\n",
    "        print(f\"Column '{date_column}' not found in DataFrame.\")\n",
    "        return df\n",
    "    \n",
    "    df = df.copy()\n",
    "    df[date_column] = pd.to_datetime(df[date_column], format='%d/%m/%Y', errors='coerce')\n",
    "    # Create int column with year, month, day, dayofweek from the real date in column date_column\n",
    "    df['year'] = df[date_column].dt.year\n",
    "    df['month'] = df[date_column].dt.month\n",
    "    df['day'] = df[date_column].dt.day\n",
    "    df['dayofweek'] = df[date_column].dt.dayofweek\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b195a4b0",
   "metadata": {},
   "source": [
    "### Covert all string columns of the dataset to strip whitespaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d31a7949",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_string_columns(df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    String manipulation: Strip whitespace from object columns\n",
    "\n",
    "    Input:\n",
    "    -------\n",
    "    - df => Pandas DataFrame to be processed\n",
    "    Output:\n",
    "    ------- \n",
    "    None (the function modifies the DataFrame in place)\n",
    "    \"\"\"\n",
    "    string_cols = df.select_dtypes(include=['object']).columns\n",
    "    for col in string_cols:\n",
    "        df[col] = df[col].str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd25a8dc",
   "metadata": {},
   "source": [
    "### Function to preprocess the initial loaded dataset: combines all the previous functions and returns a clean dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfd629e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df: pd.DataFrame,date_column: str, name_options: str =\"none\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Function to preprocess the initial loaded dataset:\n",
    "    - Strips whitespaces from strings using the convert_string_columns function. \n",
    "    - Converts \"invoice_date\" column to datetime for futural temporal manipulations.\n",
    "    - Adds \"revenue\" column derived from \"qty\" and \"amount\" columns.\n",
    "    - Create a column name using the name_treatment function correcting the first_name and last_name column\n",
    "\n",
    "    Input:\n",
    "    ---------\n",
    "    - df => Pandas DataFrame to be preprocessed\n",
    "    - date_column => Name of the column where the date is in\n",
    "    - [Optional] name_options (String) => options for the name_treatment function. Possible choixe \"none\", \"separate\", \"first\" and \"last\"\n",
    "\n",
    "    Output:\n",
    "    ---------\n",
    "    - df => Preprocessed Pandas DataFrame\n",
    "    \"\"\"\n",
    "    if 'qty' in df.columns and 'amount' in df.columns:\n",
    "        # Create 'revenue' column as product of 'quantity' and 'amount'\n",
    "        df['revenue'] = df['qty'] * df['amount']\n",
    "\n",
    "    if name_options != \"none\":\n",
    "        df = name_treatment(df,name_options)\n",
    "    df = parse_dates(df,date_column)\n",
    "    convert_string_columns(df)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8eead31",
   "metadata": {},
   "source": [
    "### Function for data exploration: displaying basic information on our dataset.\n",
    "We can see that there is no missing or NaN data since all columns have 10000 non-null rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "21203509",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_data(df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Prints key exploratory information: \n",
    "    - dataset shape (rows, columns)\n",
    "    - column data types\n",
    "    - missing values per column\n",
    "    - description of columns\n",
    "    - correlation matrix between numerical columns\n",
    "\n",
    "    Input:\n",
    "    ---------\n",
    "    - df => Pandas DataFrame to be explored\n",
    "\n",
    "    Output:\n",
    "    ---------\n",
    "    None (prints information to console)\n",
    "    \"\"\"\n",
    "    print(\"Shape (rows, columns):\", df.shape)\n",
    "\n",
    "    print(\"\\nColumn dtypes:\")\n",
    "    print(df.dtypes)\n",
    "\n",
    "    print(\"\\nMissing values per column:\")\n",
    "    print(df.isna().sum())\n",
    "\n",
    "    print(\"\\nBasic description of numerical columns:\")\n",
    "    print(df.describe())\n",
    "\n",
    "    # Correlation matrix for numeric variables\n",
    "    if 'qty' in df.columns and 'amount' in df.columns and 'revenue' in df.columns:\n",
    "        print(\"\\nCorrelation matrix (numeric columns):\")\n",
    "        print(df[['qty', 'amount', 'revenue']].corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04fbeae",
   "metadata": {},
   "source": [
    "### Testing data collection, preprocessing and exploration on the Invoices dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cdf66a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully.\n",
      "Shape (rows, columns): (10000, 16)\n",
      "\n",
      "Column dtypes:\n",
      "first_name              object\n",
      "last_name               object\n",
      "email                   object\n",
      "product_id               int64\n",
      "qty                      int64\n",
      "amount                 float64\n",
      "invoice_date    datetime64[ns]\n",
      "address                 object\n",
      "city                    object\n",
      "stock_code               int64\n",
      "job                     object\n",
      "revenue                float64\n",
      "year                     int32\n",
      "month                    int32\n",
      "day                      int32\n",
      "dayofweek                int32\n",
      "dtype: object\n",
      "\n",
      "Missing values per column:\n",
      "first_name      0\n",
      "last_name       0\n",
      "email           0\n",
      "product_id      0\n",
      "qty             0\n",
      "amount          0\n",
      "invoice_date    0\n",
      "address         0\n",
      "city            0\n",
      "stock_code      0\n",
      "job             0\n",
      "revenue         0\n",
      "year            0\n",
      "month           0\n",
      "day             0\n",
      "dayofweek       0\n",
      "dtype: int64\n",
      "\n",
      "Basic description of numerical columns:\n",
      "         product_id           qty        amount                invoice_date  \\\n",
      "count  10000.000000  10000.000000  10000.000000                       10000   \n",
      "mean     149.746700      5.005900     52.918236  1995-06-11 13:32:18.240000   \n",
      "min      100.000000      1.000000      5.010000         1970-01-05 00:00:00   \n",
      "25%      125.000000      3.000000     29.137500         1982-08-02 00:00:00   \n",
      "50%      150.000000      5.000000     53.485000         1994-12-26 00:00:00   \n",
      "75%      175.000000      7.000000     76.520000         2008-03-01 12:00:00   \n",
      "max      199.000000      9.000000     99.990000         2022-01-17 00:00:00   \n",
      "std       28.728186      2.576767     27.434579                         NaN   \n",
      "\n",
      "         stock_code       revenue          year         month           day  \\\n",
      "count  1.000000e+04  10000.000000  10000.000000  10000.000000  10000.000000   \n",
      "mean   4.950036e+07    265.687038   1994.941400      6.541900     15.869600   \n",
      "min    1.977000e+03      5.070000   1970.000000      1.000000      1.000000   \n",
      "25%    2.425234e+07     93.602500   1982.000000      4.000000      8.000000   \n",
      "50%    4.931714e+07    205.940000   1994.000000      7.000000     16.000000   \n",
      "75%    7.457446e+07    391.672500   2008.000000     10.000000     24.000000   \n",
      "max    9.999216e+07    898.920000   2022.000000     12.000000     31.000000   \n",
      "std    2.903081e+07    208.084624     14.880414      3.439975      8.867438   \n",
      "\n",
      "          dayofweek  \n",
      "count  10000.000000  \n",
      "mean       3.003200  \n",
      "min        0.000000  \n",
      "25%        1.000000  \n",
      "50%        3.000000  \n",
      "75%        5.000000  \n",
      "max        6.000000  \n",
      "std        2.009775  \n",
      "\n",
      "Correlation matrix (numeric columns):\n",
      "              qty    amount   revenue\n",
      "qty      1.000000  0.011086  0.660933\n",
      "amount   0.011086  1.000000  0.675678\n",
      "revenue  0.660933  0.675678  1.000000\n"
     ]
    }
   ],
   "source": [
    "df = load_data('invoices.csv')\n",
    "df = preprocess_data(df,date_column=\"invoice_date\")\n",
    "explore_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6922243",
   "metadata": {},
   "source": [
    "## 2) Querying the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec97e1b",
   "metadata": {},
   "source": [
    "### Indicator 1: Grouping query (top cities by total revenue)\n",
    "- Revenue = Quantity * Amount --> this is the total amount of a single transaction.\n",
    "- Identifies the most profitable geographic locations by aggregating total revenue by city.\n",
    "- Could potentially be used for business (targeted marketing, logistics, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d2d37bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indicator_top_group(df: pd.DataFrame,revenue_column: str,groupBy_column: str, n: int = 10) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute top N group by total revenue.\n",
    "\n",
    "    This function groups rows by 'groupBy_column', sums the 'revenue' values and returns the top `n` 'groupBy_column' ordered by total revenue descending.\n",
    "    \n",
    "    Input:\n",
    "    --------\n",
    "    - df: invoices dataframe\n",
    "    - revenue_column: name of the column in the dataset containing the revenue\n",
    "    - groupBy_column : name of the column in the dataset containing the variable that you want to group together\n",
    "    - n: number of 'groupBy_column'\n",
    "    \n",
    "    Output:\n",
    "    --------\n",
    "    - DataFrame with columns ['city', 'total_revenue']\n",
    "    \"\"\"\n",
    "    # Ensure revenue column exists\n",
    "    if revenue_column not in df.columns:\n",
    "        if 'qty' not in df.columns and 'amount' not in df.columns:\n",
    "            return df\n",
    "        df = df.copy()\n",
    "        df['revenue'] = df['qty'] * df['amount']\n",
    "        revenue_column = 'revenue'\n",
    "    \n",
    "    # return a dataset with the sum of the revenue and the number of transaction grouped by 'revenue_column' \n",
    "    if groupBy_column in df.columns:\n",
    "        city_rev = (\n",
    "            df.groupby(groupBy_column)\n",
    "            .agg(\n",
    "                total_revenue=(revenue_column, 'sum'),\n",
    "                transactions_count=(groupBy_column, 'count')\n",
    "            )\n",
    "            .reset_index()\n",
    "            .sort_values('total_revenue', ascending=False)\n",
    "            .head(n)\n",
    "        )\n",
    "        return city_rev\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9330e17",
   "metadata": {},
   "source": [
    "### Indicator 2: Data transformation (revenue normalization by city)\n",
    "- Apply min‑max normalization or z‑score to city revenue to compare cities independently of absolute scale.\n",
    "- Min-Max normalization using the formula:\n",
    "    $$x_{\\text{norm}} = \\frac{x - \\min(x)}{\\max(x) - \\min(x)}$$\n",
    "    where x is the original revenue and min(x), max(x) are the minimum and maximum revenues across cities.\n",
    "\n",
    "- Z-Score normalization using the formula:\n",
    "    $$z = \\frac{x - \\mu}{\\sigma}$$\n",
    "    where $\\mu$ is the mean of $x$ and $\\sigma$ is the standard deviation of $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9abd75c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_city_revenue(city_rev: pd.DataFrame, method: str = 'min-max') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Normalize total_revenue column using specified method.\n",
    "    \n",
    "    Input: \n",
    "    -------\n",
    "    - city_rev: DataFrame with 'city' and 'total_revenue'\n",
    "    - method: normalization method, either 'min-max', 'z-score' or 'both' (default 'min-max')\n",
    "    \n",
    "    Output: \n",
    "    ------\n",
    "    - same DataFrame with extra column 'revenue_norm'\n",
    "    \"\"\"\n",
    "    city_rev = city_rev.copy()\n",
    "\n",
    "    # Normalize the dataset with min-max normalization ( min_max = (x - min(x)) / (max(x)- min(x)))\n",
    "    if method in (\"minmax\", \"both\"):\n",
    "        min_val = city_rev[\"total_revenue\"].min()\n",
    "        max_val = city_rev[\"total_revenue\"].max()\n",
    "        city_rev[\"revenue_minmax\"] = (\n",
    "            (city_rev[\"total_revenue\"] - min_val) / (max_val - min_val)\n",
    "        )\n",
    "\n",
    "# Normalize the dataset with z-score normalization ( z = (x - mean(x)) / std(x))\n",
    "    if method in (\"zscore\", \"both\"):\n",
    "        mean_val = city_rev[\"total_revenue\"].mean()\n",
    "        std_val = city_rev[\"total_revenue\"].std(ddof=0)\n",
    "        if std_val != 0:\n",
    "            city_rev[\"revenue_zscore\"] = (\n",
    "                (city_rev[\"total_revenue\"] - mean_val) / std_val\n",
    "            )\n",
    "        else:\n",
    "            city_rev[\"revenue_zscore\"] = 0\n",
    "\n",
    "    return city_rev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5198cf5e",
   "metadata": {},
   "source": [
    "### Function to discretize city revenue, assigning to it revenue classes (low, medium, high). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52c5c7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize_city_revenue(city_rev: pd.DataFrame, q: int = 3) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Discretize city revenue into q quantile-based categories.\n",
    "\n",
    "    Input:\n",
    "    --------\n",
    "    - city_rev: DataFrame with 'total_revenue'\n",
    "    - q: number of bins\n",
    "    \n",
    "    Output:\n",
    "    -------\n",
    "    - DataFrame with extra column 'revenue_segment'\n",
    "    \"\"\"\n",
    "    city_rev['revenue_segment'] = pd.qcut(\n",
    "        city_rev['total_revenue'],\n",
    "        q=q,\n",
    "        labels=[f\"Segment_{i+1}\" for i in range(q)]\n",
    "    )\n",
    "    return city_rev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17d27fd",
   "metadata": {},
   "source": [
    "### Indicator 2 (Version 2): Data Transformation - Customer Segmentation\n",
    "This indicator applies MinMax Normalization to standardize features, then uses K-Means clustering to segment customers.\n",
    "\n",
    "This helps identify high-value customers for targeted marketing and retention strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "909de7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def customer_segmentation(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Applies MinMax normalization and K-Means clustering for segmentation.\n",
    "    Segments customers into Low, Medium, and High value groups based on\n",
    "    spending patterns and transaction frequency.\n",
    "\n",
    "    Input:\n",
    "    --------\n",
    "    - df: invoices dataframe\n",
    "    Output:\n",
    "    --------\n",
    "    - DataFrame with city revenue segmentation\n",
    "    \"\"\"\n",
    "    # Ensure revenue column exists\n",
    "    df = df.copy()\n",
    "    if 'revenue' not in df.columns:\n",
    "        df['revenue'] = df.get('qty', 0) * df.get('amount', 0)\n",
    "\n",
    "    # Aggregate customer-level metrics\n",
    "    customer_profile = df.groupby(['name', 'email']).agg({\n",
    "        'revenue': ['sum', 'mean', 'count'],\n",
    "        'qty': 'sum'\n",
    "    }).reset_index()\n",
    "\n",
    "    customer_profile.columns = ['name', 'email',\n",
    "                                 'total_spent', 'avg_transaction',\n",
    "                                 'num_transactions', 'total_quantity']\n",
    "    \n",
    "    # Apply MinMax Normalization to features\n",
    "    features = ['total_spent', 'avg_transaction', 'num_transactions', 'total_quantity']\n",
    "    scaler = MinMaxScaler()\n",
    "    customer_profile[[f'{f}_norm' for f in features]] = scaler.fit_transform(customer_profile[features])\n",
    "\n",
    "    # Basic safety checks before clustering\n",
    "    if customer_profile.shape[0] < 3:\n",
    "        # Not enough samples for 3 clusters: we skip clustering and return profile\n",
    "        customer_profile['segment'] = 0\n",
    "        customer_profile['segment_label'] = 'Single/Small'\n",
    "        return customer_profile\n",
    "\n",
    "    # K-Means Clustering (3 clusters)\n",
    "    kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "    customer_profile['segment'] = kmeans.fit_predict(customer_profile[['total_spent_norm', 'num_transactions_norm']])\n",
    "\n",
    "    # Label segments based on spending levels\n",
    "    segment_means = customer_profile.groupby('segment')['total_spent'].mean().sort_values()\n",
    "    segment_mapping = {\n",
    "        segment_means.index[0]: 'Low Value',\n",
    "        segment_means.index[1]: 'Medium Value',\n",
    "        segment_means.index[2]: 'High Value'\n",
    "    }\n",
    "    customer_profile['segment_label'] = customer_profile['segment'].map(segment_mapping)\n",
    "\n",
    "    print(\"Segment Distribution:\")\n",
    "    print(customer_profile['segment_label'].value_counts())\n",
    "    print(\"\\nSegment Characteristics:\")\n",
    "    print(customer_profile.groupby('segment_label')[['total_spent', 'num_transactions']].mean())\n",
    "\n",
    "    return customer_profile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b03b98",
   "metadata": {},
   "source": [
    "### Indicator 3: Temporal Analysis - Revenue Forecasting\n",
    "Function for temporal prediction of the revenue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d9de3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporal_prediction(df: pd.DataFrame, time: str=\"year\",periods: int = 10):\n",
    "    \"\"\"\n",
    "    Use Prophet model to make a temporal prediction of the revenue.\n",
    "\n",
    "    Inputs:\n",
    "    ---------\n",
    "    - df (DataFrame): Input dataset\n",
    "    - [Optionnal] time (str): options for the prediction between \"year\", \"month\" and \"day\".  \n",
    "        - \"year\" (default): use the year column of the dataset to make the prediction.  \n",
    "        - \"month\": use the month column of the dataset to make the prediction. \n",
    "        - \"day\": use the invoice_date column containing the full date to make the prediction\n",
    "    - [Optional] periods (int): The period to calculate the future date. Default 10.\n",
    "\n",
    "    Outputs: \n",
    "    --------\n",
    "    - new_df (DataFrame) - Contains 'time', 'original_revenue' and 'predicted_revenue'.\n",
    "    - model (Prophet) - Prophet model trained on the dataset and used for the prediction\n",
    "    - prediction (DataFrame) - Future prediction made by the model\n",
    "    \"\"\"\n",
    "    dataset = df.copy()\n",
    "    if time == \"year\":\n",
    "        # A mettre ailleur la transformation en datetime ?\n",
    "        dataset['year'] = pd.to_datetime(dataset['year'], format='%Y')\n",
    "\n",
    "        # Put the future date to year and group the revenue by year\n",
    "        freq = 'YE'\n",
    "        dataset = dataset.groupby('year')['revenue'].sum().reset_index()\n",
    "        # Rename the column to the name that the Prophet model will need\n",
    "        dataset.rename(columns={'year': 'ds','revenue': 'y'}, inplace=True)\n",
    "\n",
    "    elif time == \"month\":\n",
    "        # A mettre ailleur la transformation en datetime ?\n",
    "        dataset['month'] = dataset['year'].astype(str) + '-' + dataset['month'].astype(str).str.zfill(2)\n",
    "        dataset['month'] = pd.to_datetime(dataset['month'], format='%Y-%m')\n",
    "\n",
    "        # Put the future date to month and group the revenue by month\n",
    "        freq = 'ME'\n",
    "        dataset = dataset.groupby('month')['revenue'].sum().reset_index()\n",
    "        # Rename the column to the name that the Prophet model will need\n",
    "        dataset.rename(columns={'month': 'ds','revenue': 'y'}, inplace=True)\n",
    "    elif time == \"day\":\n",
    "\n",
    "        # Put the future date to day and group the revenue by day\n",
    "        freq = 'D'\n",
    "        dataset = dataset.groupby('invoice_date')['revenue'].sum().reset_index()\n",
    "        # Rename the column to the name that the Prophet model will need\n",
    "        dataset.rename(columns={'invoice_date': 'ds','revenue': 'y'}, inplace=True)\n",
    "    else:\n",
    "        print(\"Erreur: L'option 'time' doit être 'year', 'month' ou 'day.\")\n",
    "        return df,None,None\n",
    "\n",
    "    # Put cmdstanpy log ouput at ERROR to not have the output from Prophet model when it is used without trouble\n",
    "    logging.getLogger('cmdstanpy').setLevel(logging.ERROR)\n",
    "\n",
    "    # Create a Prophet model to make prediction\n",
    "    model = Prophet()\n",
    "    model.fit(dataset)\n",
    "\n",
    "    # get the date to predict and then use the predict function on these date to obtain the prediction\n",
    "    future_dates = model.make_future_dataframe(periods=periods, freq=freq)\n",
    "    prediction = model.predict(future_dates)\n",
    "\n",
    "    # Create a dataset with both original value and predicted value\n",
    "    # Then rename the new dataset columns with original_revenue and predicted_revenue\n",
    "    new_df = pd.merge(dataset[['ds', 'y']], prediction[['ds', 'yhat']], on='ds', how='outer')\n",
    "    new_df.rename(columns={'y': 'original_revenue','yhat': 'predicted_revenue', 'ds': 'time'}, inplace=True)\n",
    "    \n",
    "    return new_df, model,prediction\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e42a22",
   "metadata": {},
   "source": [
    "Function to create a visualization based on a temporal prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "480f723f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_temporal_prediction(df: pd.DataFrame, model,prediction,options: str = \"prophet\"):\n",
    "    \"\"\"\n",
    "    Create a visualization of the a dataset with temporal prediction \n",
    "    either with the dataset or with the prediction model.\n",
    "\n",
    "    Inputs:\n",
    "    ---------  \n",
    "    - df (DataFrame): Input dataset.\n",
    "    - model (Prophet): Prophet model trained on the dataset and used for the prediction.\n",
    "    - prediction (DataFrame): Future prediction made by the model.\n",
    "    - [Optional] options (str): options for the visualization between \"ploty\" and \"prophet\". \n",
    "        - \"prophet\": use prophet default plot function to plot the prediction.   \n",
    "        - \"ploty\" (default): use ploty to plot the prediction.  \n",
    "\n",
    "    Outputs: \n",
    "    --------\n",
    "    - fig (Figure) - A figure containing the temporal visualization.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a visualization using ploty express library\n",
    "    if options==\"ploty\":\n",
    "        if 'predicted_revenue' in df.columns and 'original_revenue' in df.columns:\n",
    "            fig = px.area()\n",
    "            fig.add_scatter(x=df.index, y=df[\"original_revenue\"], mode='lines', line=dict(color='blue'), name=\"original\")\n",
    "            fig.add_scatter(x=df.index ,y=df[\"predicted_revenue\"], mode='lines', line=dict(color='green'), name=\"prediction\")\n",
    "            fig.update_layout(title=\"Prediction\", xaxis_title=\"Date\", yaxis_title=\"Revenue\")\n",
    "        else:\n",
    "            print(\"Error, the prediction was not found in the dataset\")\n",
    "            fig = None\n",
    "\n",
    "    # use the plot_ploty from prohet to get a visualization\n",
    "    elif options == \"prophet\":\n",
    "        if model is not None:\n",
    "            fig = plot_plotly(model, prediction)\n",
    "        else:\n",
    "            print(\"Error, the prediction model was not found\")\n",
    "            fig = None\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2835a894",
   "metadata": {},
   "source": [
    "### Indicator 4: Spatial Analysis - Geographic Clustering\n",
    "This indicator applies K-Means clustering to group cities into activity levels based on revenue patterns (revenue = qty × amount)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afefe6d",
   "metadata": {},
   "source": [
    "Function to analyze geographic distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1797bfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_geographic_distribution(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Analyzes spatial distribution of transactions across cities.\n",
    "    Calculates revenue metrics and transaction patterns by location.\n",
    "\n",
    "    Input:\n",
    "    ---------\n",
    "    - df => Pandas DataFrame with city and revenue information\n",
    "\n",
    "    Output:\n",
    "    ---------\n",
    "    - city_stats => DataFrame with city-level statistics\n",
    "    \"\"\"\n",
    "    # Check if revenue column exists\n",
    "    if 'revenue' not in df.columns:\n",
    "        if 'qty' in df.columns and 'amount' in df.columns:\n",
    "            df['revenue'] = df['qty'] * df['amount']\n",
    "            print(\"Revenue column not found, creating it now...\")\n",
    "        else:\n",
    "            raise ValueError(\"Cannot calculate revenue: missing 'revenue' or 'qty'/'amount' columns\")\n",
    "\n",
    "    city_stats = df.groupby('city').agg({\n",
    "        'revenue': ['sum', 'mean', 'std'],\n",
    "        'qty': 'sum',\n",
    "        'product_id': 'count'\n",
    "    }).reset_index()\n",
    "\n",
    "    city_stats.columns = ['city', 'total_revenue', 'avg_revenue', 'std_revenue',\n",
    "                          'total_quantity', 'transaction_count']\n",
    "\n",
    "    city_stats['revenue_per_transaction'] = (\n",
    "        city_stats['total_revenue'] / city_stats['transaction_count'])\n",
    "\n",
    "    city_stats = city_stats.sort_values('total_revenue', ascending=False)\n",
    "\n",
    "    return city_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8db341",
   "metadata": {},
   "source": [
    "Function for spatial clustering, grouping cities by revenue patterns using KMeans clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7479e700",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatial_clustering(city_stats: pd.DataFrame, n_clusters: int = 4) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Applies K-Means clustering to group cities by revenue patterns.\n",
    "    Uses normalized features: total revenue, transaction count, and\n",
    "    average order value. Identifies geographic market segments.\n",
    "\n",
    "    Input:\n",
    "    ---------\n",
    "    - city_stats => DataFrame with city-level statistics\n",
    "    - n_clusters => Integer, number of clusters (default: 4)\n",
    "\n",
    "    Output:\n",
    "    ---------\n",
    "    - city_stats => DataFrame with cluster labels added\n",
    "    \"\"\"\n",
    "    features = ['total_revenue', 'transaction_count', 'revenue_per_transaction']\n",
    "\n",
    "    # Normalize features\n",
    "    scaler = MinMaxScaler()\n",
    "    city_stats_norm = city_stats.copy()\n",
    "    city_stats_norm[features] = scaler.fit_transform(city_stats[features])\n",
    "\n",
    "    # Apply K-Means clustering\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    city_stats['cluster'] = kmeans.fit_predict(city_stats_norm[features])\n",
    "\n",
    "    # Label clusters by activity level\n",
    "    cluster_means = city_stats.groupby('cluster')['total_revenue'].mean().sort_values()\n",
    "    cluster_labels = {\n",
    "        cluster_means.index[0]: 'Low Activity',\n",
    "        cluster_means.index[1]: 'Medium-Low Activity',\n",
    "        cluster_means.index[2]: 'Medium-High Activity',\n",
    "        cluster_means.index[3]: 'High Activity'\n",
    "    }\n",
    "    city_stats['cluster_label'] = city_stats['cluster'].map(cluster_labels)\n",
    "    \n",
    "    print(\"Cluster Distribution:\")\n",
    "    print(city_stats['cluster_label'].value_counts())\n",
    "    print(\"\\nCluster Characteristics:\")\n",
    "    print(city_stats.groupby('cluster_label')[['total_revenue', 'transaction_count']].mean())\n",
    "\n",
    "    return city_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235e9c0b",
   "metadata": {},
   "source": [
    "## 3) Dash visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e9e43354",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_indicator(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Create visualization of some indicator on the dataset.\n",
    "\n",
    "    Input:\n",
    "    ---------\n",
    "    - df => Pandas DataFrame containing our cleaned dataset\n",
    "\n",
    "    Output:\n",
    "    ---------\n",
    "    - fig_top_cities_revenue => Visualization of the city who have the N(=10) best revenue\n",
    "    - fig_customer_segmentation => Visualization of the segmentation of the customers according to 3 class\n",
    "    - figure_pred_year => Visualization of the prediction of revenue according to yearly revenue\n",
    "    - figure_pred_month => Visualization of the prediction of revenue according to monthly revenue\n",
    "    - figure_pred_day => Visualization of the prediction of revenue according to daily revenue\n",
    "    - fig_spatial_clustering => Visualization of the city cluster by revenue and activity\n",
    "\n",
    "    \"\"\"\n",
    "                                        ### Indicator 1\n",
    "\n",
    "    # Creating the indicator\n",
    "    city_stats = indicator_top_group(df,revenue_column=\"revenue\",groupBy_column=\"city\", n=10)\n",
    "\n",
    "    # Creating the vizualization\n",
    "    fig_top_cities_revenue = px.bar(\n",
    "        city_stats,\n",
    "        x='city',\n",
    "        y='total_revenue',\n",
    "        hover_data={'transactions_count': True, 'total_revenue': ':.2f'},\n",
    "        title='Top Cities by Revenue and Transactions',\n",
    "        labels={'total_revenue': 'Total Revenue ($)', 'city': 'City Name'},\n",
    "        color='transactions_count',\n",
    "        color_continuous_scale='Blues'\n",
    "    )\n",
    "    fig_top_cities_revenue.update_layout(\n",
    "        xaxis_tickangle=-45,\n",
    "        height=400\n",
    "    )\n",
    "\n",
    "                                        ### Indicator 2\n",
    "    \n",
    "    # Creating the indicator\n",
    "    customer_segments = customer_segmentation(df)\n",
    "\n",
    "    # Creating the vizualization\n",
    "    fig_customer_segmentation = go.Figure([\n",
    "        go.Pie(\n",
    "            labels=customer_segments['segment_label'].value_counts().index,\n",
    "            values=customer_segments['segment_label'].value_counts().values,\n",
    "            hole=0.4,\n",
    "            marker=dict(colors=['red', 'orange', 'green'])\n",
    "        )\n",
    "    ]).update_layout(\n",
    "        title='Customer Distribution by Segment (Low, Medium or High Value)',\n",
    "        height=400\n",
    "    )\n",
    "\n",
    "                                        ### Indicator 3\n",
    "\n",
    "    ## To change the data used to make the prediction change time_pred,\n",
    "    ## Put the oldest date that you want to be taken in year.\n",
    "    ## In case you change time_pred and use the create_dashboard function,\n",
    "    ## change the year in year_to_index and the label in the Dropdown\n",
    "    time_pred = [1900,2020,2019,2017,2015,2010,2000,1990,1980]\n",
    "\n",
    "    ## To change how far the model should predict, change the prediction_lenght variable,\n",
    "    ## It must be a positive int number\n",
    "    prediction_lenght_year = 10\n",
    "    prediction_lenght_month = 24\n",
    "    prediction_lenght_day = 120\n",
    "\n",
    "    figure_pred_year = []\n",
    "    figure_pred_month = []\n",
    "    figure_pred_day = []\n",
    "\n",
    "    # Make the prediction using yearly, monthly and Daily data for every date in time_pred\n",
    "    for i in time_pred:\n",
    "        dataset = df.copy()\n",
    "        dataset = dataset[dataset[\"year\"]>i]\n",
    "        data_y, model_y,predictions_y = temporal_prediction(dataset,time=\"year\", periods=prediction_lenght_year)\n",
    "        figure_pred_year.append(display_temporal_prediction(data_y,model_y,predictions_y))\n",
    "        data_m,model_m,predictions_m = temporal_prediction(dataset,time=\"month\",periods=prediction_lenght_month)\n",
    "        figure_pred_month.append(display_temporal_prediction(data_m,model_m,predictions_m))\n",
    "        data_m,model_m,predictions_m = temporal_prediction(dataset,time=\"day\",periods=prediction_lenght_day)\n",
    "        figure_pred_day.append(display_temporal_prediction(data_m,model_m,predictions_m))\n",
    "\n",
    "                                        ### Indicator 4\n",
    "\n",
    "    # Creating the indicator\n",
    "    city_clusters = spatial_clustering(analyze_geographic_distribution(df))\n",
    "    \n",
    "    # Creating the vizualization\n",
    "    fig_spatial_clustering = px.scatter(\n",
    "        city_clusters.head(3000),\n",
    "        x='transaction_count',\n",
    "        y='total_revenue',\n",
    "        color='cluster_label',\n",
    "        size='revenue_per_transaction',\n",
    "        hover_data=['city'],\n",
    "        title='City Clustering by Revenue and Activity',\n",
    "        labels={\n",
    "            'transaction_count': 'Number of Transactions',\n",
    "            'total_revenue': 'Total Revenue ($)',\n",
    "            'cluster_label': 'Activity Level'\n",
    "        },\n",
    "        color_discrete_map={\n",
    "            'Low Activity': 'red',\n",
    "            'Medium-Low Activity': 'orange',\n",
    "            'Medium-High Activity': 'blue',\n",
    "            'High Activity': 'green'\n",
    "        }\n",
    "    ).update_layout(height=400)\n",
    "\n",
    "    return fig_top_cities_revenue,fig_customer_segmentation,figure_pred_year,figure_pred_month,figure_pred_day,fig_spatial_clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "69840c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dashboard(df: pd.DataFrame) -> Dash:\n",
    "    \n",
    "    # Getting the visualization\n",
    "    fig_top_cities_revenue,fig_customer_segmentation,figure_pred_year,figure_pred_month,figure_pred_day,fig_spatial_clustering = create_indicator(df)\n",
    "\n",
    "    # Dictionnary to transform the markdown value in indicator 3 \n",
    "    # to an index that can be used in update_temporal_graph function\n",
    "    ## This variable work with the Dropdown options in the dasboard and the update_temporal_graph function \n",
    "    year_to_index = {\n",
    "    \"YA\": 0, \"MA\": 0, \"DA\": 0,    # 1900 (All)\n",
    "    \"Y2020\": 1, \"M2020\": 1, \"D2020\": 1,\n",
    "    \"Y2019\": 2, \"M2019\": 2, \"D2019\": 2,\n",
    "    \"Y2017\": 3, \"M2017\": 3, \"D2017\": 3,\n",
    "    \"Y2015\": 4, \"M2015\": 4, \"D2015\": 4,\n",
    "    \"Y2010\": 5, \"M2010\": 5, \"D2010\": 5,\n",
    "    \"Y2000\": 6, \"M2000\": 6, \"D2000\": 6,\n",
    "    \"Y1990\": 7, \"M1990\": 7, \"D1990\": 7,\n",
    "    \"Y1980\": 8, \"M1980\": 8, \"D1980\": 8}\n",
    "\n",
    "    # Initialize the Dash app\n",
    "    app = Dash(__name__)\n",
    "\n",
    "    app.layout = html.Div([\n",
    "        # Header\n",
    "        html.Div([\n",
    "            html.H1('Invoices Data Analysis Dashboard', \n",
    "                    style={'textAlign': 'center', 'color': '#2c3e50', 'marginBottom': 10}),\n",
    "            html.H3('Programming for Data Science - Final Project', \n",
    "                    style={'textAlign': 'center', 'color': '#34495e', 'marginBottom': 5}),\n",
    "            html.P('Team Members: Alvaro SERERO, Leo WINTER, Yoann SUBLET, Kellian VERVAELE KLEIN',\n",
    "                style={'textAlign': 'center', 'color': '#7f8c8d', 'fontSize': 14}),\n",
    "            html.P('Dataset: Invoices (Kaggle) - 10,000 online store transactions',\n",
    "                style={'textAlign': 'center', 'color': '#7f8c8d', 'fontSize': 14, 'marginBottom': 20}),\n",
    "            html.Div([\n",
    "                html.P('Project Objective: Extract and visualize four key business indicators from invoice data ' +\n",
    "                    'to support data-driven decision-making in e-commerce operations.',\n",
    "                    style={'textAlign': 'center', 'color': '#2c3e50', 'fontSize': 15, \n",
    "                            'maxWidth': '900px', 'margin': '0 auto', 'padding': '15px',\n",
    "                            'backgroundColor': '#ecf0f1', 'borderRadius': '8px'})\n",
    "            ])\n",
    "        ], style={'backgroundColor': '#f8f9fa', 'padding': '20px', 'marginBottom': '30px',\n",
    "                'borderRadius': '10px', 'boxShadow': '0 2px 4px rgba(0,0,0,0.1)'}),\n",
    "\n",
    "        # Dashboard content\n",
    "        html.Div([\n",
    "            # Row 1: Indicators 1 and 2\n",
    "            html.Div([\n",
    "                # Indicator 1: Top Cities by Revenue and Transactions\n",
    "                html.Div([\n",
    "                    html.H3('Indicator 1: Top Cities by Total Revenue', \n",
    "                            style={'color': '#2980b9', 'marginBottom': 15}),\n",
    "                    html.P('Grouping Query - Aggregates total revenue and transaction count per city',\n",
    "                        style={'fontSize': 13, 'color': '#7f8c8d', 'marginBottom': 15}),\n",
    "                    dcc.Graph(\n",
    "                        figure=fig_top_cities_revenue\n",
    "                    )\n",
    "                ], style={'width': '48%', 'display': 'inline-block', 'verticalAlign': 'top',\n",
    "                        'padding': '20px', 'backgroundColor': 'white', 'borderRadius': '8px',\n",
    "                        'boxShadow': '0 2px 4px rgba(0,0,0,0.1)', 'marginRight': '2%'}),\n",
    "                \n",
    "                # Indicator 2: Customer Segmentation\n",
    "                html.Div([\n",
    "                    html.H3('Indicator 2: Customer Segmentation', \n",
    "                            style={'color': '#27ae60', 'marginBottom': 15}),\n",
    "                    html.P('Data Transformation - MinMax Normalization + K-Means Clustering',\n",
    "                        style={'fontSize': 13, 'color': '#7f8c8d', 'marginBottom': 15}),\n",
    "                    dcc.Graph(\n",
    "                        figure=fig_customer_segmentation\n",
    "                    )\n",
    "                ], style={'width': '48%', 'display': 'inline-block', 'verticalAlign': 'top',\n",
    "                        'padding': '20px', 'backgroundColor': 'white', 'borderRadius': '8px',\n",
    "                        'boxShadow': '0 2px 4px rgba(0,0,0,0.1)'})\n",
    "            ], style={'marginBottom': '30px'}),  # End of Row 1\n",
    "\n",
    "            # Row 2: Indicators 3 and 4\n",
    "            html.Div([\n",
    "                # Indicator 3: Revenue Prediction\n",
    "                html.Div([\n",
    "                    html.H3('Indicator 3: Prediction of future revenue', \n",
    "                        style={'color': '#2980b9', 'marginBottom': 15}),\n",
    "                    html.P('Predict the future by taking information from past time',\n",
    "                        style={'fontSize': 13, 'color': '#7f8c8d', 'marginBottom': 15}),\n",
    "                    dcc.Graph(id='graph'),\n",
    "                    dcc.Dropdown(options=[{\"label\": \"All Year\", \"value\": \"YA\"},{\"label\": \"2020 Year\", \"value\": \"Y2020\"},\n",
    "                                        {\"label\": \"2019 Year\", \"value\": \"Y2019\"},{\"label\": \"2017 Year\", \"value\": \"Y2017\"},\n",
    "                                        {\"label\": \"2015 Year\", \"value\": \"Y2015\"},{\"label\": \"2010 Year\", \"value\": \"Y2010\"},\n",
    "                                        {\"label\": \"2000 Year\", \"value\": \"Y2000\"},{\"label\": \"1990 Year\", \"value\": \"Y1990\"},\n",
    "                                        {\"label\": \"1980 Year\", \"value\": \"Y1980\"},\n",
    "                                        {\"label\": \"All Month\", \"value\": \"MA\"},{\"label\": \"2020 Month\", \"value\": \"M2020\"},\n",
    "                                        {\"label\": \"2019 Month\", \"value\": \"M2019\"},{\"label\": \"2017 Month\", \"value\": \"M2017\"},\n",
    "                                        {\"label\": \"2015 Month\", \"value\": \"M2015\"},{\"label\": \"2010 Month\", \"value\": \"M2010\"},\n",
    "                                        {\"label\": \"2000 Month\", \"value\": \"M2000\"},{\"label\": \"1990 Month\", \"value\": \"M1990\"},\n",
    "                                        {\"label\": \"1980 Month\", \"value\": \"M1980\"},\n",
    "                                        {\"label\": \"All Day\", \"value\": \"DA\"},{\"label\": \"2020 Day\", \"value\": \"D2020\"},\n",
    "                                        {\"label\": \"2019 Day\", \"value\": \"D2019\"},{\"label\": \"2017 Day\", \"value\": \"D2017\"},\n",
    "                                        {\"label\": \"2015 Day\", \"value\": \"D2015\"},{\"label\": \"2010 Day\", \"value\": \"D2010\"},\n",
    "                                        {\"label\": \"2000 Day\", \"value\": \"D2000\"},{\"label\": \"1990 Day\", \"value\": \"D1990\"},\n",
    "                                        {\"label\": \"1980 Day\", \"value\": \"D1980\"}],\n",
    "                                            value=\"YA\", id='dropdown')\n",
    "                ]),\n",
    "\n",
    "                # Indicator 4: Spatial Clustering\n",
    "                html.Div([\n",
    "                    html.H3('Indicator 4: Geographic Clustering (Spatial)', \n",
    "                            style={'color': '#e67e22', 'marginBottom': 15}),\n",
    "                    html.P('Spatial Analysis - K-Means Clustering of cities by activity level',\n",
    "                        style={'fontSize': 13, 'color': '#7f8c8d', 'marginBottom': 10}),\n",
    "                    dcc.Graph(\n",
    "                        figure=fig_spatial_clustering\n",
    "                    )\n",
    "                ], style={'width': '48%', 'display': 'inline-block', 'verticalAlign': 'top',\n",
    "                        'padding': '20px', 'backgroundColor': 'white', 'borderRadius': '8px',\n",
    "                        'boxShadow': '0 2px 4px rgba(0,0,0,0.1)'})\n",
    "            ]),\n",
    "        ], style={'padding': '20px', 'maxWidth': '1400px', 'margin': '0 auto'}),\n",
    "    ])\n",
    "\n",
    "    # callback to get the information from the indicator 3 dropdown\n",
    "    @callback(\n",
    "    Output('graph', 'figure'),\n",
    "    Input('dropdown', 'value'))\n",
    "    # Function that work with the callback by taking \n",
    "    # the dropdown value and returning the graph for indicator 3\n",
    "    # input : the \"value\" variable of the dropdown id='dropdown'\n",
    "    # output : the visualization containing revenue prediction\n",
    "    def update_temporal_graph(selected_value):\n",
    "        index = year_to_index[selected_value]\n",
    "        if selected_value.startswith(\"Y\"):\n",
    "            fig_dash = figure_pred_year[index]\n",
    "        elif selected_value.startswith(\"M\"):\n",
    "            fig_dash = figure_pred_month[index]\n",
    "        elif selected_value.startswith(\"D\"):\n",
    "            fig_dash = figure_pred_day[index]\n",
    "        return fig_dash\n",
    "    return app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113feb76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully.\n",
      "Data loaded successfully.\n",
      "Shape (rows, columns): (20000, 15)\n",
      "\n",
      "Column dtypes:\n",
      "name                    object\n",
      "address                 object\n",
      "amount                 float64\n",
      "city                    object\n",
      "email                   object\n",
      "invoice_date    datetime64[ns]\n",
      "job                     object\n",
      "product_id               int64\n",
      "qty                      int64\n",
      "revenue                float64\n",
      "stock_code               int64\n",
      "year                     int32\n",
      "month                    int32\n",
      "day                      int32\n",
      "dayofweek                int32\n",
      "dtype: object\n",
      "\n",
      "Missing values per column:\n",
      "name            0\n",
      "address         0\n",
      "amount          0\n",
      "city            0\n",
      "email           0\n",
      "invoice_date    0\n",
      "job             0\n",
      "product_id      0\n",
      "qty             0\n",
      "revenue         0\n",
      "stock_code      0\n",
      "year            0\n",
      "month           0\n",
      "day             0\n",
      "dayofweek       0\n",
      "dtype: int64\n",
      "\n",
      "Basic description of numerical columns:\n",
      "             amount                invoice_date    product_id           qty  \\\n",
      "count  20000.000000                       20000  20000.000000  20000.000000   \n",
      "mean      52.918236  1995-06-11 13:32:18.240000    149.746700      5.005900   \n",
      "min        5.010000         1970-01-05 00:00:00    100.000000      1.000000   \n",
      "25%       29.137500         1982-08-02 00:00:00    125.000000      3.000000   \n",
      "50%       53.485000         1994-12-26 00:00:00    150.000000      5.000000   \n",
      "75%       76.520000         2008-03-01 12:00:00    175.000000      7.000000   \n",
      "max       99.990000         2022-01-17 00:00:00    199.000000      9.000000   \n",
      "std       27.433893                         NaN     28.727468      2.576703   \n",
      "\n",
      "            revenue    stock_code          year         month           day  \\\n",
      "count  20000.000000  2.000000e+04  20000.000000  20000.000000  20000.000000   \n",
      "mean     265.687038  4.950036e+07   1994.941400      6.541900     15.869600   \n",
      "min        5.070000  1.977000e+03   1970.000000      1.000000      1.000000   \n",
      "25%       93.602500  2.425234e+07   1982.000000      4.000000      8.000000   \n",
      "50%      205.940000  4.931714e+07   1994.000000      7.000000     16.000000   \n",
      "75%      391.672500  7.457446e+07   2008.000000     10.000000     24.000000   \n",
      "max      898.920000  9.999216e+07   2022.000000     12.000000     31.000000   \n",
      "std      208.079422  2.903008e+07     14.880042      3.439889      8.867216   \n",
      "\n",
      "          dayofweek  \n",
      "count  20000.000000  \n",
      "mean       3.003200  \n",
      "min        0.000000  \n",
      "25%        1.000000  \n",
      "50%        3.000000  \n",
      "75%        5.000000  \n",
      "max        6.000000  \n",
      "std        2.009724  \n",
      "\n",
      "Correlation matrix (numeric columns):\n",
      "              qty    amount   revenue\n",
      "qty      1.000000  0.011086  0.660933\n",
      "amount   0.011086  1.000000  0.675678\n",
      "revenue  0.660933  0.675678  1.000000\n",
      "Segment Distribution:\n",
      "segment_label\n",
      "Low Value       10574\n",
      "Medium Value     6188\n",
      "High Value       3238\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Segment Characteristics:\n",
      "               total_spent  num_transactions\n",
      "segment_label                               \n",
      "High Value      639.697912               1.0\n",
      "Low Value       106.796832               1.0\n",
      "Medium Value    341.488239               1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:21:51 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:21:52 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    file_path = \"invoices.csv\"\n",
    "    # https://www.kaggle.com/datasets/ghassenkhaled/invoices-data\n",
    "    file_path_extra = \"invoices_extra.csv\" \n",
    "    df = load_data(file_path)\n",
    "    df_extra = load_data(file_path_extra)\n",
    "    df = preprocess_data(df,date_column=\"invoice_date\", name_options=\"separate\")\n",
    "    explore_data(df)\n",
    "\n",
    "    app = create_dashboard(df)\n",
    "    app.run()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2038daae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully.\n",
      "Shape (rows, columns): (10000, 15)\n",
      "\n",
      "Column dtypes:\n",
      "id_invoice                int64\n",
      "issuedDate       datetime64[ns]\n",
      "country                  object\n",
      "service                  object\n",
      "total                   float64\n",
      "discount                float64\n",
      "tax                     float64\n",
      "invoiceStatus            object\n",
      "balance                 float64\n",
      "dueDate                  object\n",
      "client                   object\n",
      "year                    float64\n",
      "month                   float64\n",
      "day                     float64\n",
      "dayofweek               float64\n",
      "dtype: object\n",
      "\n",
      "Missing values per column:\n",
      "id_invoice           0\n",
      "issuedDate       10000\n",
      "country              0\n",
      "service              0\n",
      "total                0\n",
      "discount             0\n",
      "tax                  0\n",
      "invoiceStatus        0\n",
      "balance              0\n",
      "dueDate              0\n",
      "client               0\n",
      "year             10000\n",
      "month            10000\n",
      "day              10000\n",
      "dayofweek        10000\n",
      "dtype: int64\n",
      "\n",
      "Basic description of numerical columns:\n",
      "        id_invoice issuedDate         total      discount           tax  \\\n",
      "count  10000.00000          0  10000.000000  10000.000000  10000.000000   \n",
      "mean      50.47610        NaT   3337.413969    332.527743    166.439248   \n",
      "min        1.00000        NaT    200.480000      0.040000      0.000000   \n",
      "25%       25.00000        NaT   1375.182500     81.067500     41.172500   \n",
      "50%       50.00000        NaT   2864.520000    217.935000    112.610000   \n",
      "75%       76.00000        NaT   4696.567500    479.220000    240.067500   \n",
      "max      100.00000        NaT   9997.620000   1938.560000    972.980000   \n",
      "std       29.02202        NaN   2353.530270    332.979375    165.802507   \n",
      "\n",
      "            balance  year  month  day  dayofweek  \n",
      "count  10000.000000   0.0    0.0  0.0        0.0  \n",
      "mean    3171.325474   NaN    NaN  NaN        NaN  \n",
      "min      169.020000   NaN    NaN  NaN        NaN  \n",
      "25%     1322.912500   NaN    NaN  NaN        NaN  \n",
      "50%     2713.915000   NaN    NaN  NaN        NaN  \n",
      "75%     4445.292500   NaN    NaN  NaN        NaN  \n",
      "max    10691.480000   NaN    NaN  NaN        NaN  \n",
      "std     2250.604297   NaN    NaN  NaN        NaN  \n",
      "\n",
      "Correlation matrix (numeric columns):\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['qty', 'amount', 'revenue'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m df_extra = load_data(file_path_extra)\n\u001b[32m      3\u001b[39m df_extra = preprocess_data(df_extra,date_column=\u001b[33m\"\u001b[39m\u001b[33missuedDate\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mexplore_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_extra\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m df_extra\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 31\u001b[39m, in \u001b[36mexplore_data\u001b[39m\u001b[34m(df)\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Correlation matrix for numeric variables\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mCorrelation matrix (numeric columns):\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mqty\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mamount\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrevenue\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m.corr())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\leoma\\Documents\\anaconda3\\envs\\prophet_env\\Lib\\site-packages\\pandas\\core\\frame.py:4119\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4117\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[32m   4118\u001b[39m         key = \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m4119\u001b[39m     indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcolumns\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m]\n\u001b[32m   4121\u001b[39m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[32m   4122\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[38;5;28mbool\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\leoma\\Documents\\anaconda3\\envs\\prophet_env\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6212\u001b[39m, in \u001b[36mIndex._get_indexer_strict\u001b[39m\u001b[34m(self, key, axis_name)\u001b[39m\n\u001b[32m   6209\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6210\u001b[39m     keyarr, indexer, new_indexer = \u001b[38;5;28mself\u001b[39m._reindex_non_unique(keyarr)\n\u001b[32m-> \u001b[39m\u001b[32m6212\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6214\u001b[39m keyarr = \u001b[38;5;28mself\u001b[39m.take(indexer)\n\u001b[32m   6215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[32m   6216\u001b[39m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\leoma\\Documents\\anaconda3\\envs\\prophet_env\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6261\u001b[39m, in \u001b[36mIndex._raise_if_missing\u001b[39m\u001b[34m(self, key, indexer, axis_name)\u001b[39m\n\u001b[32m   6259\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[32m   6260\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m nmissing == \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[32m-> \u001b[39m\u001b[32m6261\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   6263\u001b[39m     not_found = \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask.nonzero()[\u001b[32m0\u001b[39m]].unique())\n\u001b[32m   6264\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in index\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: \"None of [Index(['qty', 'amount', 'revenue'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "file_path_extra = \"invoices_extra.csv\" \n",
    "df_extra = load_data(file_path_extra)\n",
    "df_extra = preprocess_data(df_extra,date_column=\"issuedDate\")\n",
    "explore_data(df_extra)\n",
    "df_extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d138a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prophet_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
