{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78cb3d7c",
   "metadata": {},
   "source": [
    "# Programming in Data Science - Final Project\n",
    "## Invoices Dataset Analysis\n",
    "**Team Members: Leo WINTER, Yoann SUBLET, Kellian VERVAELE KLEIN, Alvaro SERERO**\n",
    "\n",
    "Dataset: Invoices (Kaggle)\n",
    "\n",
    "Source: https://www.kaggle.com/datasets/cankatsrc/invoices/data\n",
    "\n",
    "This dataset includes multiple fields such as customer details (first name, last name, email), transaction information (product ID, quantity, amount, invoice date), and additional attributes like address, city, and stock code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cb51c4",
   "metadata": {},
   "source": [
    "### Import all needed libraries for the project:\n",
    "- Pandas for data manipulation\n",
    "- Plotly express for visualizations\n",
    "- Dash for creating a visual and interactive dashboard interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fcdd6c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from dash import Dash, dcc, html,Input, Output\n",
    "from dash import callback\n",
    "from prophet import Prophet\n",
    "from prophet.plot import plot_plotly\n",
    "import logging\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324203ca",
   "metadata": {},
   "source": [
    "## 1) Data collection and exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a26496a",
   "metadata": {},
   "source": [
    "### Function to safely load CSV data from a file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3179ec2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Function to load CSV data from a given file path safely.\n",
    "\n",
    "    Input: \n",
    "    ------\n",
    "    file_path => String, path to the CSV file\n",
    "\n",
    "    Output: \n",
    "    ------\n",
    "    dataset => pd.DataFrame containing the loaded data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(\"Data loaded successfully.\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d423915",
   "metadata": {},
   "source": [
    "### Function to process invalid first_name and last_name columns.\n",
    "- In the initial dataset there are \"last_name\" and \"first_name\" columns but each one contains a combination of a first name and last name which does not make sense since a trasaction is only made by one individual person and columns should contain exactly what is described by their name.\n",
    "- For example, the first line is structured as follows, which is a mistake and need to be corrected.\n",
    "\n",
    "first_name | last_name  \n",
    "Carmen Nixon | Todd Anderson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d601f854",
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_treatment(dataset: pd.DataFrame, options: str=\"separate\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Function to treat first_name and last_name columns in a dataset.\n",
    "    \n",
    "    Input:\n",
    "    ---------\n",
    "    - dataset => Pandas DataFrame, dataset must have first_name and last_name columns\n",
    "    - options => String, options for treatment between \"separate\", \"first\" and \"last\"\n",
    "        - \"separate\" (default): create two new line for each name, \n",
    "        - \"first\": keep only the first_name renamed as name, \n",
    "        - \"last\" : keep only the last_name renamed as name\n",
    "    \n",
    "    Output:\n",
    "    ---------\n",
    "    - dataset => Pandas DataFrame after treating first_name and last_name columns\n",
    "    \"\"\"\n",
    "\n",
    "    if \"first_name\" in dataset and \"last_name\" in dataset:\n",
    "        if options == \"separate\":\n",
    "            value = dataset.columns.difference(['first_name','last_name']).tolist()\n",
    "            new_dataset = pd.melt(dataset, id_vars=value,              \n",
    "                              value_vars=['first_name', 'last_name'],\n",
    "                              value_name='name')\n",
    "            \n",
    "            autres_colonnes = [col for col in new_dataset.columns if col not in [\"name\", \"variable\"]]\n",
    "            nouvel_ordre = [\"name\"] + autres_colonnes\n",
    "            new_dataset = new_dataset[nouvel_ordre]\n",
    "\n",
    "        elif options == \"first\":\n",
    "            new_dataset = dataset.drop(columns=['last_name'])\n",
    "            new_dataset.rename(columns={'first_name': 'name'}, inplace=True)\n",
    "\n",
    "        elif options == \"last\":\n",
    "            new_dataset = dataset.drop(columns=['first_name'])\n",
    "            new_dataset.rename(columns={'last_name': 'name'}, inplace=True)\n",
    "        else:\n",
    "            print(f\"{options} is not a correct parameters of options, please write 'separate' or 'first' or 'last\")\n",
    "            return dataset\n",
    "        return new_dataset\n",
    "    else:\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26aeba0c",
   "metadata": {},
   "source": [
    "### Function to parse invoice dates:\n",
    "- Convert \"invoice_date\" column to datetime for futural temporal manipulations.\n",
    "- Extracts year, month, day, and day of week features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bdbddd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_dates(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Converts \"invoice_date\" column from string to datetime.\n",
    "    Extracts year, month, day, and day of week features.\n",
    "\n",
    "    Input:\n",
    "    ------\n",
    "    - df (DataFrame) - dataset with invoice_date column\n",
    "\n",
    "    Output:\n",
    "    ------\n",
    "    - df (DataFrame) - dataset with parsed datetime features\n",
    "    \"\"\"\n",
    "    if 'invoice_date' not in df.columns:\n",
    "        print(\"Column 'invoice_date' not found in DataFrame.\")\n",
    "        return df\n",
    "    \n",
    "    df = df.copy()\n",
    "    df['invoice_date'] = pd.to_datetime(df['invoice_date'], format='%d/%m/%Y', errors='coerce')\n",
    "    df['year'] = df['invoice_date'].dt.year\n",
    "    df['month'] = df['invoice_date'].dt.month\n",
    "    df['day'] = df['invoice_date'].dt.day\n",
    "    df['dayofweek'] = df['invoice_date'].dt.dayofweek\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b195a4b0",
   "metadata": {},
   "source": [
    "### Covert all string columns of the dataset to strip whitespaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d31a7949",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_string_columns(df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    String manipulation: Strip whitespace from object columns\n",
    "\n",
    "    Input:\n",
    "    -------\n",
    "    - df => Pandas DataFrame to be processed\n",
    "    Output:\n",
    "    ------- \n",
    "    None (the function modifies the DataFrame in place)\n",
    "    \"\"\"\n",
    "    string_cols = df.select_dtypes(include=['object']).columns\n",
    "    for col in string_cols:\n",
    "        df[col] = df[col].str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd25a8dc",
   "metadata": {},
   "source": [
    "### Function to preprocess the initial loaded dataset: combines all the previous functions and returns a clean dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "adfd629e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df: pd.DataFrame, name_options: str =\"separate\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Function to preprocess the initial loaded dataset:\n",
    "    - Strips whitespaces from strings using the convert_string_columns function. \n",
    "    - Converts \"invoice_date\" column to datetime for futural temporal manipulations.\n",
    "    - Adds \"revenue\" column derived from \"qty\" and \"amount\" columns.\n",
    "    - Create a column name using the name_treatment function correcting the first_name and last_name column\n",
    "\n",
    "    Input:\n",
    "    ---------\n",
    "    - df => Pandas DataFrame to be preprocessed\n",
    "    - [Optional] name_options (String) => options for the name_treatment function. Possible choixe \"separate\", \"first\" and \"last\"\n",
    "    Output:\n",
    "    ---------\n",
    "    - df => Preprocessed Pandas DataFrame\n",
    "    \"\"\"\n",
    "    if 'qty' in df.columns and 'amount' in df.columns:\n",
    "        # Create 'revenue' column as product of 'quantity' and 'amount'\n",
    "        df['revenue'] = df['qty'] * df['amount']\n",
    "\n",
    "    df = name_treatment(df, options=name_options)\n",
    "    df = parse_dates(df)\n",
    "    convert_string_columns(df)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8eead31",
   "metadata": {},
   "source": [
    "### Function for data exploration: displaying basic information on our dataset.\n",
    "We can see that there is no missing or NaN data since all columns have 10000 non-null rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "21203509",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_data(df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Prints key exploratory information: \n",
    "    - dataset shape (rows, columns)\n",
    "    - column data types\n",
    "    - missing values per column\n",
    "    - description of columns\n",
    "    - correlation matrix between numerical columns\n",
    "\n",
    "    Input:\n",
    "    ---------\n",
    "    - df => Pandas DataFrame to be explored\n",
    "\n",
    "    Output:\n",
    "    ---------\n",
    "    None (prints information to console)\n",
    "    \"\"\"\n",
    "    print(\"Shape (rows, columns):\", df.shape)\n",
    "\n",
    "    print(\"\\nColumn dtypes:\")\n",
    "    print(df.dtypes)\n",
    "\n",
    "    print(\"\\nMissing values per column:\")\n",
    "    print(df.isna().sum())\n",
    "\n",
    "    print(\"\\nBasic description of numerical columns:\")\n",
    "    print(df.describe())\n",
    "\n",
    "    # Correlation matrix for numeric variables\n",
    "    print(\"\\nCorrelation matrix (numeric columns):\")\n",
    "    print(df[['qty', 'amount', 'revenue']].corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04fbeae",
   "metadata": {},
   "source": [
    "### Testing data collection, preprocessing and exploration on the Invoices dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cdf66a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully.\n",
      "Shape (rows, columns): (20000, 15)\n",
      "\n",
      "Column dtypes:\n",
      "name                    object\n",
      "address                 object\n",
      "amount                 float64\n",
      "city                    object\n",
      "email                   object\n",
      "invoice_date    datetime64[ns]\n",
      "job                     object\n",
      "product_id               int64\n",
      "qty                      int64\n",
      "revenue                float64\n",
      "stock_code               int64\n",
      "year                     int32\n",
      "month                    int32\n",
      "day                      int32\n",
      "dayofweek                int32\n",
      "dtype: object\n",
      "\n",
      "Missing values per column:\n",
      "name            0\n",
      "address         0\n",
      "amount          0\n",
      "city            0\n",
      "email           0\n",
      "invoice_date    0\n",
      "job             0\n",
      "product_id      0\n",
      "qty             0\n",
      "revenue         0\n",
      "stock_code      0\n",
      "year            0\n",
      "month           0\n",
      "day             0\n",
      "dayofweek       0\n",
      "dtype: int64\n",
      "\n",
      "Basic description of numerical columns:\n",
      "             amount                invoice_date    product_id           qty  \\\n",
      "count  20000.000000                       20000  20000.000000  20000.000000   \n",
      "mean      52.918236  1995-06-11 13:32:18.240000    149.746700      5.005900   \n",
      "min        5.010000         1970-01-05 00:00:00    100.000000      1.000000   \n",
      "25%       29.137500         1982-08-02 00:00:00    125.000000      3.000000   \n",
      "50%       53.485000         1994-12-26 00:00:00    150.000000      5.000000   \n",
      "75%       76.520000         2008-03-01 12:00:00    175.000000      7.000000   \n",
      "max       99.990000         2022-01-17 00:00:00    199.000000      9.000000   \n",
      "std       27.433893                         NaN     28.727468      2.576703   \n",
      "\n",
      "            revenue    stock_code          year         month           day  \\\n",
      "count  20000.000000  2.000000e+04  20000.000000  20000.000000  20000.000000   \n",
      "mean     265.687038  4.950036e+07   1994.941400      6.541900     15.869600   \n",
      "min        5.070000  1.977000e+03   1970.000000      1.000000      1.000000   \n",
      "25%       93.602500  2.425234e+07   1982.000000      4.000000      8.000000   \n",
      "50%      205.940000  4.931714e+07   1994.000000      7.000000     16.000000   \n",
      "75%      391.672500  7.457446e+07   2008.000000     10.000000     24.000000   \n",
      "max      898.920000  9.999216e+07   2022.000000     12.000000     31.000000   \n",
      "std      208.079422  2.903008e+07     14.880042      3.439889      8.867216   \n",
      "\n",
      "          dayofweek  \n",
      "count  20000.000000  \n",
      "mean       3.003200  \n",
      "min        0.000000  \n",
      "25%        1.000000  \n",
      "50%        3.000000  \n",
      "75%        5.000000  \n",
      "max        6.000000  \n",
      "std        2.009724  \n",
      "\n",
      "Correlation matrix (numeric columns):\n",
      "              qty    amount   revenue\n",
      "qty      1.000000  0.011086  0.660933\n",
      "amount   0.011086  1.000000  0.675678\n",
      "revenue  0.660933  0.675678  1.000000\n"
     ]
    }
   ],
   "source": [
    "df = load_data('invoices.csv')\n",
    "df = preprocess_data(df)\n",
    "explore_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6922243",
   "metadata": {},
   "source": [
    "## 2) Querying the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec97e1b",
   "metadata": {},
   "source": [
    "### Indicator 1: Grouping query (top cities by total revenue)\n",
    "- Revenue = Quantity * Amount --> this is the total amount of a single transaction.\n",
    "- Identifies the most profitable geographic locations by aggregating total revenue by city.\n",
    "- Could potentially be used for business (targeted marketing, logistics, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d2d37bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indicator_top_cities(df: pd.DataFrame, n: int = 10) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute top N cities by total revenue.\n",
    "\n",
    "    This function groups rows by city, sums the 'revenue' values and returns the top `n` cities ordered by total revenue descending.\n",
    "    \n",
    "    Input:\n",
    "        - df: invoices dataframe\n",
    "        - n: number of cities\n",
    "    \n",
    "    Output:\n",
    "        - DataFrame with columns ['city', 'total_revenue']\n",
    "    \"\"\"\n",
    "    # Ensure revenue column exists\n",
    "    if 'revenue' not in df.columns:\n",
    "        df = df.copy()\n",
    "        df['revenue'] = df['qty'] * df['amount']\n",
    "    \n",
    "    city_rev = (\n",
    "        df.groupby('city')\n",
    "        .agg(\n",
    "            total_revenue=('revenue', 'sum'),\n",
    "            transactions_count=('city', 'count')\n",
    "        )\n",
    "        .reset_index()\n",
    "        .sort_values('total_revenue', ascending=False)\n",
    "        .head(n)\n",
    "    )\n",
    "    return city_rev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9330e17",
   "metadata": {},
   "source": [
    "### Indicator 2: Data transformation (revenue normalization by city)\n",
    "- Apply min‑max normalization or z‑score to city revenue to compare cities independently of absolute scale.\n",
    "- Min-Max normalization using the formula:\n",
    "    $$x_{\\text{norm}} = \\frac{x - \\min(x)}{\\max(x) - \\min(x)}$$\n",
    "    where x is the original revenue and min(x), max(x) are the minimum and maximum revenues across cities.\n",
    "\n",
    "- Z-Score normalization using the formula:\n",
    "    $$z = \\frac{x - \\mu}{\\sigma}$$\n",
    "    where $\\mu$ is the mean of $x$ and $\\sigma$ is the standard deviation of $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9abd75c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_city_revenue(city_rev: pd.DataFrame, method: str = 'min-max') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Normalize total_revenue column using specified method.\n",
    "    \n",
    "    Input: \n",
    "        - city_rev: DataFrame with 'city' and 'total_revenue'\n",
    "        - method: normalization method, either 'min-max', 'z-score' or 'both' (default 'min-max')\n",
    "    \n",
    "    Output: \n",
    "        - same DataFrame with extra column 'revenue_norm'\n",
    "    \"\"\"\n",
    "    city_rev = city_rev.copy()\n",
    "\n",
    "    if method in (\"minmax\", \"both\"):\n",
    "        min_val = city_rev[\"total_revenue\"].min()\n",
    "        max_val = city_rev[\"total_revenue\"].max()\n",
    "        city_rev[\"revenue_minmax\"] = (\n",
    "            (city_rev[\"total_revenue\"] - min_val) / (max_val - min_val)\n",
    "        )\n",
    "\n",
    "    if method in (\"zscore\", \"both\"):\n",
    "        mean_val = city_rev[\"total_revenue\"].mean()\n",
    "        std_val = city_rev[\"total_revenue\"].std(ddof=0)\n",
    "        if std_val != 0:\n",
    "            city_rev[\"revenue_zscore\"] = (\n",
    "                (city_rev[\"total_revenue\"] - mean_val) / std_val\n",
    "            )\n",
    "        else:\n",
    "            city_rev[\"revenue_zscore\"] = 0\n",
    "\n",
    "    return city_rev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5198cf5e",
   "metadata": {},
   "source": [
    "### Function to discretize city revenue, assigning to it revenue classes (low, medium, high). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "52c5c7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize_city_revenue(city_rev: pd.DataFrame, q: int = 3) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Discretize city revenue into q quantile-based categories.\n",
    "\n",
    "    Input:\n",
    "        - city_rev: DataFrame with 'total_revenue'\n",
    "        - q: number of bins\n",
    "    \n",
    "    Output:\n",
    "        - DataFrame with extra column 'revenue_segment'\n",
    "    \"\"\"\n",
    "    city_rev['revenue_segment'] = pd.qcut(\n",
    "        city_rev['total_revenue'],\n",
    "        q=q,\n",
    "        labels=[f\"Segment_{i+1}\" for i in range(q)]\n",
    "    )\n",
    "    return city_rev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17d27fd",
   "metadata": {},
   "source": [
    "### Indicator 2 (Version 2): Data Transformation - Customer Segmentation\n",
    "This indicator applies MinMax Normalization to standardize features, then uses K-Means clustering to segment customers.\n",
    "\n",
    "This helps identify high-value customers for targeted marketing and retention strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "909de7e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segment Distribution:\n",
      "segment_label\n",
      "Low Value       10574\n",
      "Medium Value     6188\n",
      "High Value       3238\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Segment Characteristics:\n",
      "               total_spent  num_transactions\n",
      "segment_label                               \n",
      "High Value      639.697912               1.0\n",
      "Low Value       106.796832               1.0\n",
      "Medium Value    341.488239               1.0\n"
     ]
    }
   ],
   "source": [
    "def customer_segmentation(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Applies MinMax normalization and K-Means clustering for segmentation.\n",
    "    Segments customers into Low, Medium, and High value groups based on\n",
    "    spending patterns and transaction frequency.\n",
    "\n",
    "    Input:\n",
    "        - df: invoices dataframe\n",
    "    Output:\n",
    "        - DataFrame with city revenue segmentation\n",
    "    \"\"\"\n",
    "    # Ensure revenue column exists\n",
    "    df = df.copy()\n",
    "    if 'revenue' not in df.columns:\n",
    "        df['revenue'] = df.get('qty', 0) * df.get('amount', 0)\n",
    "\n",
    "    # Aggregate customer-level metrics\n",
    "    customer_profile = df.groupby(['name', 'email']).agg({\n",
    "        'revenue': ['sum', 'mean', 'count'],\n",
    "        'qty': 'sum'\n",
    "    }).reset_index()\n",
    "\n",
    "    customer_profile.columns = ['name', 'email',\n",
    "                                 'total_spent', 'avg_transaction',\n",
    "                                 'num_transactions', 'total_quantity']\n",
    "    \n",
    "    # Apply MinMax Normalization to features\n",
    "    features = ['total_spent', 'avg_transaction', 'num_transactions', 'total_quantity']\n",
    "    scaler = MinMaxScaler()\n",
    "    customer_profile[[f'{f}_norm' for f in features]] = scaler.fit_transform(customer_profile[features])\n",
    "\n",
    "    # Basic safety checks before clustering\n",
    "    if customer_profile.shape[0] < 3:\n",
    "        # Not enough samples for 3 clusters: we skip clustering and return profile\n",
    "        customer_profile['segment'] = 0\n",
    "        customer_profile['segment_label'] = 'Single/Small'\n",
    "        return customer_profile\n",
    "\n",
    "    # K-Means Clustering (3 clusters)\n",
    "    kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "    customer_profile['segment'] = kmeans.fit_predict(customer_profile[['total_spent_norm', 'num_transactions_norm']])\n",
    "\n",
    "    # Label segments based on spending levels\n",
    "    segment_means = customer_profile.groupby('segment')['total_spent'].mean().sort_values()\n",
    "    segment_mapping = {\n",
    "        segment_means.index[0]: 'Low Value',\n",
    "        segment_means.index[1]: 'Medium Value',\n",
    "        segment_means.index[2]: 'High Value'\n",
    "    }\n",
    "    customer_profile['segment_label'] = customer_profile['segment'].map(segment_mapping)\n",
    "\n",
    "    print(\"Segment Distribution:\")\n",
    "    print(customer_profile['segment_label'].value_counts())\n",
    "    print(\"\\nSegment Characteristics:\")\n",
    "    print(customer_profile.groupby('segment_label')[['total_spent', 'num_transactions']].mean())\n",
    "\n",
    "    return customer_profile\n",
    "\n",
    "customer_segments = customer_segmentation(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b03b98",
   "metadata": {},
   "source": [
    "### Indicator 3: Temporal Analysis - Revenue Forecasting\n",
    "Function for temporal prediction of the revenue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9de3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporal_prediction(df: pd.DataFrame, time: str=\"year\",periods: int = 10):\n",
    "    \"\"\"\n",
    "    Use Prophet model to make a temporal prediction of the revenue.\n",
    "\n",
    "    Inputs:\n",
    "    ---------\n",
    "    - df (DataFrame): Input dataset\n",
    "    - [Optionnal] time (str): options for the prediction between \"year\", \"month\" and \"day\".  \n",
    "        - \"year\" (default): use the year column of the dataset to make the prediction.  \n",
    "        - \"month\": use the month column of the dataset to make the prediction. \n",
    "        - \"day\": use the invoice_date column containing the full date to make the prediction\n",
    "    - [Optional] periods (int): The period to calculate the future date. Default 10.\n",
    "\n",
    "    Outputs: \n",
    "    --------\n",
    "    - new_df (DataFrame) - Contains 'time', 'original_revenue' and 'predicted_revenue'.\n",
    "    - model (Prophet) - Prophet model trained on the dataset and used for the prediction\n",
    "    - prediction (DataFrame) - Future prediction made by the model\n",
    "    \"\"\"\n",
    "    dataset = df.copy()\n",
    "    if time == \"year\":\n",
    "        # A mettre ailleur la transformation en datetime ?\n",
    "        dataset['year'] = pd.to_datetime(dataset['year'], format='%Y')\n",
    "\n",
    "        freq = 'YE'\n",
    "        dataset = dataset.groupby('year')['revenue'].sum().reset_index()\n",
    "        dataset.rename(columns={'year': 'ds','revenue': 'y'}, inplace=True)\n",
    "\n",
    "    elif time == \"month\":\n",
    "        # A mettre ailleur la transformation en datetime ?\n",
    "        dataset['month'] = dataset['year'].astype(str) + '-' + dataset['month'].astype(str).str.zfill(2)\n",
    "        dataset['month'] = pd.to_datetime(dataset['month'], format='%Y-%m')\n",
    "\n",
    "        freq = 'ME'\n",
    "        dataset = dataset.groupby('month')['revenue'].sum().reset_index()\n",
    "        dataset.rename(columns={'month': 'ds','revenue': 'y'}, inplace=True)\n",
    "    elif time == \"day\":\n",
    "        freq = 'D'\n",
    "\n",
    "        dataset = dataset.groupby('invoice_date')['revenue'].sum().reset_index()\n",
    "        dataset.rename(columns={'invoice_date': 'ds','revenue': 'y'}, inplace=True)\n",
    "    else:\n",
    "        print(\"Erreur: L'option 'time' doit être 'year', 'month' ou 'day.\")\n",
    "        return df,None,None\n",
    "\n",
    "    # Put cmdstanpy log ouput at ERROR to not have the output when the function is used\n",
    "    logging.getLogger('cmdstanpy').setLevel(logging.ERROR)\n",
    "\n",
    "    # Create a Prophet model to make prediction\n",
    "    model = Prophet()\n",
    "    model.fit(dataset)\n",
    "\n",
    "    future_dates = model.make_future_dataframe(periods=periods, freq=freq)\n",
    "    prediction = model.predict(future_dates)\n",
    "\n",
    "    new_df = pd.merge(dataset[['ds', 'y']], prediction[['ds', 'yhat']], on='ds', how='outer')\n",
    "    new_df.rename(columns={'y': 'original_revenue','yhat': 'predicted_revenue', 'ds': 'time'}, inplace=True)\n",
    "    \n",
    "    return new_df, model,prediction\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e42a22",
   "metadata": {},
   "source": [
    "Function to create a visualization based on a temporal prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "480f723f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_temporal_prediction(df: pd.DataFrame, model,prediction,options: str = \"prophet\"):\n",
    "    \"\"\"\n",
    "    Create a visualization of the a dataset with temporal prediction \n",
    "    either with the dataset or with the prediction model.\n",
    "\n",
    "    Inputs:\n",
    "    ---------  \n",
    "    - df (DataFrame): Input dataset.\n",
    "    - model (Prophet): Prophet model trained on the dataset and used for the prediction.\n",
    "    - prediction (DataFrame): Future prediction made by the model.\n",
    "    - [Optional] options (str): options for the visualization between \"ploty\" and \"prophet\". \n",
    "        - \"prophet\": use prophet default plot function to plot the prediction.   \n",
    "        - \"ploty\" (default): use ploty to plot the prediction.  \n",
    "\n",
    "    Outputs: \n",
    "    --------\n",
    "    - fig (Figure) - A figure containing the temporal visualization.\n",
    "    \"\"\"\n",
    "    if options==\"ploty\":\n",
    "        if 'predicted_revenue' in df.columns and 'original_revenue' in df.columns:\n",
    "            fig = px.area()\n",
    "            fig.add_scatter(x=df.index, y=df[\"original_revenue\"], mode='lines', line=dict(color='blue'), name=\"original\")\n",
    "            fig.add_scatter(x=df.index ,y=df[\"predicted_revenue\"], mode='lines', line=dict(color='green'), name=\"prediction\")\n",
    "            fig.update_layout(title=\"Prediction\", xaxis_title=\"Date\", yaxis_title=\"Revenue\")\n",
    "        else:\n",
    "            print(\"Error, the prediction  was not found in the dataset\")\n",
    "            fig = None\n",
    "\n",
    "    elif options == \"prophet\":\n",
    "        if model is not None:\n",
    "            fig = plot_plotly(model, prediction)\n",
    "        else:\n",
    "            print(\"Error, the prediction model was not found\")\n",
    "            fig = None\n",
    "\n",
    "    # elif options == \"3\":\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2835a894",
   "metadata": {},
   "source": [
    "### Indicator 4: Spatial Analysis - Geographic Clustering\n",
    "This indicator applies K-Means clustering to group cities into activity levels based on revenue patterns (revenue = qty × amount)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afefe6d",
   "metadata": {},
   "source": [
    "Function to analyze geographic distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1797bfb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>total_revenue</th>\n",
       "      <th>avg_revenue</th>\n",
       "      <th>std_revenue</th>\n",
       "      <th>total_quantity</th>\n",
       "      <th>transaction_count</th>\n",
       "      <th>revenue_per_transaction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2864</th>\n",
       "      <td>Lake James</td>\n",
       "      <td>8834.34</td>\n",
       "      <td>368.097500</td>\n",
       "      <td>224.764250</td>\n",
       "      <td>130</td>\n",
       "      <td>24</td>\n",
       "      <td>368.097500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5395</th>\n",
       "      <td>Port Kimberly</td>\n",
       "      <td>6288.20</td>\n",
       "      <td>524.016667</td>\n",
       "      <td>304.185069</td>\n",
       "      <td>76</td>\n",
       "      <td>12</td>\n",
       "      <td>524.016667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4741</th>\n",
       "      <td>North Michael</td>\n",
       "      <td>5804.54</td>\n",
       "      <td>362.783750</td>\n",
       "      <td>266.636647</td>\n",
       "      <td>100</td>\n",
       "      <td>16</td>\n",
       "      <td>362.783750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6105</th>\n",
       "      <td>Smithmouth</td>\n",
       "      <td>5289.06</td>\n",
       "      <td>330.566250</td>\n",
       "      <td>266.060729</td>\n",
       "      <td>82</td>\n",
       "      <td>16</td>\n",
       "      <td>330.566250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6351</th>\n",
       "      <td>South Jennifer</td>\n",
       "      <td>5140.66</td>\n",
       "      <td>285.592222</td>\n",
       "      <td>181.955704</td>\n",
       "      <td>94</td>\n",
       "      <td>18</td>\n",
       "      <td>285.592222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2387</th>\n",
       "      <td>Johnstonchester</td>\n",
       "      <td>10.88</td>\n",
       "      <td>5.440000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5.440000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6577</th>\n",
       "      <td>South Sandraberg</td>\n",
       "      <td>10.68</td>\n",
       "      <td>5.340000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5.340000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>Cameronborough</td>\n",
       "      <td>10.22</td>\n",
       "      <td>5.110000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5.110000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6882</th>\n",
       "      <td>Torresfurt</td>\n",
       "      <td>10.16</td>\n",
       "      <td>5.080000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5.080000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1303</th>\n",
       "      <td>East Josephtown</td>\n",
       "      <td>10.14</td>\n",
       "      <td>5.070000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5.070000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7773 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  city  total_revenue  avg_revenue  std_revenue  \\\n",
       "2864        Lake James        8834.34   368.097500   224.764250   \n",
       "5395     Port Kimberly        6288.20   524.016667   304.185069   \n",
       "4741     North Michael        5804.54   362.783750   266.636647   \n",
       "6105        Smithmouth        5289.06   330.566250   266.060729   \n",
       "6351    South Jennifer        5140.66   285.592222   181.955704   \n",
       "...                ...            ...          ...          ...   \n",
       "2387   Johnstonchester          10.88     5.440000     0.000000   \n",
       "6577  South Sandraberg          10.68     5.340000     0.000000   \n",
       "544     Cameronborough          10.22     5.110000     0.000000   \n",
       "6882        Torresfurt          10.16     5.080000     0.000000   \n",
       "1303   East Josephtown          10.14     5.070000     0.000000   \n",
       "\n",
       "      total_quantity  transaction_count  revenue_per_transaction  \n",
       "2864             130                 24               368.097500  \n",
       "5395              76                 12               524.016667  \n",
       "4741             100                 16               362.783750  \n",
       "6105              82                 16               330.566250  \n",
       "6351              94                 18               285.592222  \n",
       "...              ...                ...                      ...  \n",
       "2387               2                  2                 5.440000  \n",
       "6577               2                  2                 5.340000  \n",
       "544                2                  2                 5.110000  \n",
       "6882               2                  2                 5.080000  \n",
       "1303               2                  2                 5.070000  \n",
       "\n",
       "[7773 rows x 7 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def analyze_geographic_distribution(df):\n",
    "    \"\"\"\n",
    "    Analyzes spatial distribution of transactions across cities.\n",
    "    Calculates revenue metrics and transaction patterns by location.\n",
    "\n",
    "    Input:\n",
    "    ---------\n",
    "    - df => Pandas DataFrame with city and revenue information\n",
    "\n",
    "    Output:\n",
    "    ---------\n",
    "    - city_stats => DataFrame with city-level statistics\n",
    "    \"\"\"\n",
    "    # Check if revenue column exists\n",
    "    if 'revenue' not in df.columns:\n",
    "        if 'qty' in df.columns and 'amount' in df.columns:\n",
    "            df['revenue'] = df['qty'] * df['amount']\n",
    "            print(\"Revenue column not found, creating it now...\")\n",
    "        else:\n",
    "            raise ValueError(\"Cannot calculate revenue: missing 'revenue' or 'qty'/'amount' columns\")\n",
    "\n",
    "    city_stats = df.groupby('city').agg({\n",
    "        'revenue': ['sum', 'mean', 'std'],\n",
    "        'qty': 'sum',\n",
    "        'product_id': 'count'\n",
    "    }).reset_index()\n",
    "\n",
    "    city_stats.columns = ['city', 'total_revenue', 'avg_revenue', 'std_revenue',\n",
    "                          'total_quantity', 'transaction_count']\n",
    "\n",
    "    city_stats['revenue_per_transaction'] = (\n",
    "        city_stats['total_revenue'] / city_stats['transaction_count'])\n",
    "\n",
    "    city_stats = city_stats.sort_values('total_revenue', ascending=False)\n",
    "\n",
    "    return city_stats\n",
    "\n",
    "analyze_geographic_distribution(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8db341",
   "metadata": {},
   "source": [
    "Function for spatial clustering, grouping cities by revenue patterns using KMeans clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7479e700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Distribution:\n",
      "cluster_label\n",
      "Low Activity            3809\n",
      "Medium-Low Activity     2377\n",
      "Medium-High Activity    1182\n",
      "High Activity            405\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Cluster Characteristics:\n",
      "                      total_revenue  transaction_count\n",
      "cluster_label                                         \n",
      "High Activity           2172.758765           7.664198\n",
      "Low Activity             252.502331           2.268837\n",
      "Medium-High Activity    1338.599915           2.159052\n",
      "Medium-Low Activity      795.021868           2.398822\n"
     ]
    }
   ],
   "source": [
    "def spatial_clustering(city_stats, n_clusters=4):\n",
    "    \"\"\"\n",
    "    Applies K-Means clustering to group cities by revenue patterns.\n",
    "    Uses normalized features: total revenue, transaction count, and\n",
    "    average order value. Identifies geographic market segments.\n",
    "\n",
    "    Input:\n",
    "    ---------\n",
    "    - city_stats => DataFrame with city-level statistics\n",
    "    - n_clusters => Integer, number of clusters (default: 4)\n",
    "\n",
    "    Output:\n",
    "    ---------\n",
    "    - city_stats => DataFrame with cluster labels added\n",
    "    \"\"\"\n",
    "    features = ['total_revenue', 'transaction_count', 'revenue_per_transaction']\n",
    "\n",
    "    # Normalize features\n",
    "    scaler = MinMaxScaler()\n",
    "    city_stats_norm = city_stats.copy()\n",
    "    city_stats_norm[features] = scaler.fit_transform(city_stats[features])\n",
    "\n",
    "    # Apply K-Means clustering\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    city_stats['cluster'] = kmeans.fit_predict(city_stats_norm[features])\n",
    "\n",
    "    # Label clusters by activity level\n",
    "    cluster_means = city_stats.groupby('cluster')['total_revenue'].mean().sort_values()\n",
    "    cluster_labels = {\n",
    "        cluster_means.index[0]: 'Low Activity',\n",
    "        cluster_means.index[1]: 'Medium-Low Activity',\n",
    "        cluster_means.index[2]: 'Medium-High Activity',\n",
    "        cluster_means.index[3]: 'High Activity'\n",
    "    }\n",
    "    city_stats['cluster_label'] = city_stats['cluster'].map(cluster_labels)\n",
    "    \n",
    "    print(\"Cluster Distribution:\")\n",
    "    print(city_stats['cluster_label'].value_counts())\n",
    "    print(\"\\nCluster Characteristics:\")\n",
    "    print(city_stats.groupby('cluster_label')[['total_revenue', 'transaction_count']].mean())\n",
    "\n",
    "    return city_stats\n",
    "\n",
    "city_clusters = spatial_clustering(analyze_geographic_distribution(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235e9c0b",
   "metadata": {},
   "source": [
    "## 3) Dash visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "69840c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dashboard(df: pd.DataFrame) -> Dash:\n",
    "    # Indicator 1\n",
    "    city_stats = indicator_top_cities(df, n=10)\n",
    "\n",
    "    fig_top_cities_revenue = px.bar(\n",
    "        city_stats,\n",
    "        x='city',\n",
    "        y='total_revenue',\n",
    "        hover_data={'transactions_count': True, 'total_revenue': ':.2f'},\n",
    "        title='Top Cities by Revenue and Transactions',\n",
    "        labels={'total_revenue': 'Total Revenue ($)', 'city': 'City Name'},\n",
    "        color='transactions_count',\n",
    "        color_continuous_scale='Blues'\n",
    "    )\n",
    "    fig_top_cities_revenue.update_layout(\n",
    "        xaxis_tickangle=-45,\n",
    "        height=400\n",
    "    )\n",
    "\n",
    "    fig_top_cities_transactions = px.scatter(\n",
    "        city_stats,\n",
    "        x='transactions_count',\n",
    "        y='total_revenue',\n",
    "        size='transactions_count',\n",
    "        color='total_revenue',\n",
    "        color_continuous_scale='Viridis',\n",
    "        hover_data={'city': True, 'transactions_count': True, 'total_revenue': ':.2f'},\n",
    "        labels={'transactions_count': 'Transactions', 'total_revenue': 'Total Revenue ($)'},\n",
    "        title='Total Revenue vs Number of Transactions (per City)'\n",
    "    )\n",
    "\n",
    "    # Indicator 2\n",
    "    fig_customer_segmentation = go.Figure([\n",
    "        go.Pie(\n",
    "            labels=customer_segments['segment_label'].value_counts().index,\n",
    "            values=customer_segments['segment_label'].value_counts().values,\n",
    "            hole=0.4,\n",
    "            marker=dict(colors=['red', 'orange', 'green'])\n",
    "        )\n",
    "    ]).update_layout(\n",
    "        title='Customer Distribution by Segment (Low, Medium or High Value)',\n",
    "        height=400\n",
    "    )\n",
    "\n",
    "    # Indicator 3\n",
    "    figure_pred_year = []\n",
    "    figure_pred_month = []\n",
    "    figure_pred_day = []\n",
    "    time_pred = [1900,2020,2019,2017,2015,2010,2000,1990,1980]\n",
    "    for i in time_pred:\n",
    "        dataset = df.copy()\n",
    "        dataset = dataset[dataset[\"year\"]>i]\n",
    "        data_y, model_y,predictions_y = temporal_prediction(dataset,time=\"year\")\n",
    "        figure_pred_year.append(display_temporal_prediction(data_y,model_y,predictions_y))\n",
    "        data_m,model_m,predictions_m = temporal_prediction(dataset,time=\"month\",periods=12)\n",
    "        figure_pred_month.append(display_temporal_prediction(data_m,model_m,predictions_m))\n",
    "        data_m,model_m,predictions_m = temporal_prediction(dataset,time=\"day\",periods=120)\n",
    "        figure_pred_day.append(display_temporal_prediction(data_m,model_m,predictions_m))\n",
    "    year_to_index = {\n",
    "    \"YA\": 0, \"MA\": 0, \"DA\": 0,    # 1900 (All)\n",
    "    \"Y2020\": 1, \"M2020\": 1, \"D2020\": 1,\n",
    "    \"Y2019\": 2, \"M2019\": 2, \"D2019\": 2,\n",
    "    \"Y2017\": 3, \"M2017\": 3, \"D2017\": 3,\n",
    "    \"Y2015\": 4, \"M2015\": 4, \"D2015\": 4,\n",
    "    \"Y2010\": 5, \"M2010\": 5, \"D2010\": 5,\n",
    "    \"Y2000\": 6, \"M2000\": 6, \"D2000\": 6,\n",
    "    \"Y1990\": 7, \"M1990\": 7, \"D1990\": 7,\n",
    "    \"Y1980\": 8, \"M1980\": 8, \"D1980\": 8}\n",
    "\n",
    "    # Indicator 4\n",
    "    fig_spatial_clustering = px.scatter(\n",
    "        city_clusters.head(3000),\n",
    "        x='transaction_count',\n",
    "        y='total_revenue',\n",
    "        color='cluster_label',\n",
    "        size='revenue_per_transaction',\n",
    "        hover_data=['city'],\n",
    "        title='City Clustering by Revenue and Activity',\n",
    "        labels={\n",
    "            'transaction_count': 'Number of Transactions',\n",
    "            'total_revenue': 'Total Revenue ($)',\n",
    "            'cluster_label': 'Activity Level'\n",
    "        },\n",
    "        color_discrete_map={\n",
    "            'Low Activity': 'red',\n",
    "            'Medium-Low Activity': 'orange',\n",
    "            'Medium-High Activity': 'blue',\n",
    "            'High Activity': 'green'\n",
    "        }\n",
    "    ).update_layout(height=400)\n",
    "    \n",
    "    # Initialize the Dash app\n",
    "    app = Dash(__name__)\n",
    "\n",
    "    app.layout = html.Div([\n",
    "        # Header\n",
    "        html.Div([\n",
    "            html.H1('Invoices Data Analysis Dashboard', \n",
    "                    style={'textAlign': 'center', 'color': '#2c3e50', 'marginBottom': 10}),\n",
    "            html.H3('Programming for Data Science - Final Project', \n",
    "                    style={'textAlign': 'center', 'color': '#34495e', 'marginBottom': 5}),\n",
    "            html.P('Team Members: Alvaro SERERO, Leo WINTER, Yoann SUBLET, Kellian VERVAELE KLEIN',\n",
    "                style={'textAlign': 'center', 'color': '#7f8c8d', 'fontSize': 14}),\n",
    "            html.P('Dataset: Invoices (Kaggle) - 10,000 online store transactions',\n",
    "                style={'textAlign': 'center', 'color': '#7f8c8d', 'fontSize': 14, 'marginBottom': 20}),\n",
    "            html.Div([\n",
    "                html.P('Project Objective: Extract and visualize four key business indicators from invoice data ' +\n",
    "                    'to support data-driven decision-making in e-commerce operations.',\n",
    "                    style={'textAlign': 'center', 'color': '#2c3e50', 'fontSize': 15, \n",
    "                            'maxWidth': '900px', 'margin': '0 auto', 'padding': '15px',\n",
    "                            'backgroundColor': '#ecf0f1', 'borderRadius': '8px'})\n",
    "            ])\n",
    "        ], style={'backgroundColor': '#f8f9fa', 'padding': '20px', 'marginBottom': '30px',\n",
    "                'borderRadius': '10px', 'boxShadow': '0 2px 4px rgba(0,0,0,0.1)'}),\n",
    "\n",
    "        # Dashboard content\n",
    "        html.Div([\n",
    "            # Row 1: Indicators 1 and 2\n",
    "            html.Div([\n",
    "                # Indicator 1: Top Cities by Revenue and Transactions\n",
    "                html.Div([\n",
    "                    html.H3('Indicator 1: Top Cities by Total Revenue', \n",
    "                            style={'color': '#2980b9', 'marginBottom': 15}),\n",
    "                    html.P('Grouping Query - Aggregates total revenue and transaction count per city',\n",
    "                        style={'fontSize': 13, 'color': '#7f8c8d', 'marginBottom': 15}),\n",
    "                    dcc.Graph(\n",
    "                        figure=fig_top_cities_revenue\n",
    "                    )\n",
    "                ], style={'width': '48%', 'display': 'inline-block', 'verticalAlign': 'top',\n",
    "                        'padding': '20px', 'backgroundColor': 'white', 'borderRadius': '8px',\n",
    "                        'boxShadow': '0 2px 4px rgba(0,0,0,0.1)', 'marginRight': '2%'}),\n",
    "                \n",
    "                # Indicator 2: Customer Segmentation\n",
    "                html.Div([\n",
    "                    html.H3('Indicator 2: Customer Segmentation', \n",
    "                            style={'color': '#27ae60', 'marginBottom': 15}),\n",
    "                    html.P('Data Transformation - MinMax Normalization + K-Means Clustering',\n",
    "                        style={'fontSize': 13, 'color': '#7f8c8d', 'marginBottom': 15}),\n",
    "                    dcc.Graph(\n",
    "                        figure=fig_customer_segmentation\n",
    "                    )\n",
    "                ], style={'width': '48%', 'display': 'inline-block', 'verticalAlign': 'top',\n",
    "                        'padding': '20px', 'backgroundColor': 'white', 'borderRadius': '8px',\n",
    "                        'boxShadow': '0 2px 4px rgba(0,0,0,0.1)'})\n",
    "            ], style={'marginBottom': '30px'}),  # End of Row 1\n",
    "\n",
    "            # Row 2: Indicators 3 and 4\n",
    "            html.Div([\n",
    "                # Indicator 3: Revenue Prediction\n",
    "                html.Div([\n",
    "                    html.H3('Indicator 3: Prediction of future revenue', \n",
    "                        style={'color': '#2980b9', 'marginBottom': 15}),\n",
    "                    html.P('Predict the future by taking information from past time',\n",
    "                        style={'fontSize': 13, 'color': '#7f8c8d', 'marginBottom': 15}),\n",
    "                    dcc.Graph(id='graph'),\n",
    "                    dcc.Dropdown(options=[{\"label\": \"All Year\", \"value\": \"YA\"},{\"label\": \"2020 Year\", \"value\": \"Y2020\"},\n",
    "                                        {\"label\": \"2019 Year\", \"value\": \"Y2019\"},{\"label\": \"2017 Year\", \"value\": \"Y2017\"},\n",
    "                                        {\"label\": \"2015 Year\", \"value\": \"Y2015\"},{\"label\": \"2010 Year\", \"value\": \"Y2010\"},\n",
    "                                        {\"label\": \"2000 Year\", \"value\": \"Y2000\"},{\"label\": \"1990 Year\", \"value\": \"Y1990\"},\n",
    "                                        {\"label\": \"1980 Year\", \"value\": \"Y1980\"},\n",
    "                                        {\"label\": \"All Month\", \"value\": \"MA\"},{\"label\": \"2020 Month\", \"value\": \"M2020\"},\n",
    "                                        {\"label\": \"2019 Month\", \"value\": \"M2019\"},{\"label\": \"2017 Month\", \"value\": \"M2017\"},\n",
    "                                        {\"label\": \"2015 Month\", \"value\": \"M2015\"},{\"label\": \"2010 Month\", \"value\": \"M2010\"},\n",
    "                                        {\"label\": \"2000 Month\", \"value\": \"M2000\"},{\"label\": \"1990 Month\", \"value\": \"M1990\"},\n",
    "                                        {\"label\": \"1980 Month\", \"value\": \"M1980\"},\n",
    "                                        {\"label\": \"All Day\", \"value\": \"DA\"},{\"label\": \"2020 Day\", \"value\": \"D2020\"},\n",
    "                                        {\"label\": \"2019 Day\", \"value\": \"D2019\"},{\"label\": \"2017 Day\", \"value\": \"D2017\"},\n",
    "                                        {\"label\": \"2015 Day\", \"value\": \"D2015\"},{\"label\": \"2010 Day\", \"value\": \"D2010\"},\n",
    "                                        {\"label\": \"2000 Day\", \"value\": \"D2000\"},{\"label\": \"1990 Day\", \"value\": \"D1990\"},\n",
    "                                        {\"label\": \"1980 Day\", \"value\": \"D1980\"}],\n",
    "                                            value=\"YA\", id='dropdown')\n",
    "                ]),\n",
    "\n",
    "                # Indicator 4: Spatial Clustering\n",
    "                html.Div([\n",
    "                    html.H3('Indicator 4: Geographic Clustering (Spatial)', \n",
    "                            style={'color': '#e67e22', 'marginBottom': 15}),\n",
    "                    html.P('Spatial Analysis - K-Means Clustering of cities by activity level',\n",
    "                        style={'fontSize': 13, 'color': '#7f8c8d', 'marginBottom': 10}),\n",
    "                    dcc.Graph(\n",
    "                        figure=fig_spatial_clustering\n",
    "                    )\n",
    "                ], style={'width': '48%', 'display': 'inline-block', 'verticalAlign': 'top',\n",
    "                        'padding': '20px', 'backgroundColor': 'white', 'borderRadius': '8px',\n",
    "                        'boxShadow': '0 2px 4px rgba(0,0,0,0.1)'})\n",
    "            ]),\n",
    "        ], style={'padding': '20px', 'maxWidth': '1400px', 'margin': '0 auto'}),\n",
    "    ])\n",
    "\n",
    "    @callback(\n",
    "    Output('graph', 'figure'),\n",
    "    Input('dropdown', 'value'))\n",
    "    def update_temporal_graph(selected_value):\n",
    "        index = year_to_index[selected_value]\n",
    "        if selected_value.startswith(\"Y\"):\n",
    "            fig_dash = figure_pred_year[index]\n",
    "        elif selected_value.startswith(\"M\"):\n",
    "            fig_dash = figure_pred_month[index]\n",
    "        elif selected_value.startswith(\"D\"):\n",
    "            fig_dash = figure_pred_day[index]\n",
    "        return fig_dash\n",
    "    return app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "113feb76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully.\n",
      "Shape (rows, columns): (20000, 15)\n",
      "\n",
      "Column dtypes:\n",
      "name                    object\n",
      "address                 object\n",
      "amount                 float64\n",
      "city                    object\n",
      "email                   object\n",
      "invoice_date    datetime64[ns]\n",
      "job                     object\n",
      "product_id               int64\n",
      "qty                      int64\n",
      "revenue                float64\n",
      "stock_code               int64\n",
      "year                     int32\n",
      "month                    int32\n",
      "day                      int32\n",
      "dayofweek                int32\n",
      "dtype: object\n",
      "\n",
      "Missing values per column:\n",
      "name            0\n",
      "address         0\n",
      "amount          0\n",
      "city            0\n",
      "email           0\n",
      "invoice_date    0\n",
      "job             0\n",
      "product_id      0\n",
      "qty             0\n",
      "revenue         0\n",
      "stock_code      0\n",
      "year            0\n",
      "month           0\n",
      "day             0\n",
      "dayofweek       0\n",
      "dtype: int64\n",
      "\n",
      "Basic description of numerical columns:\n",
      "             amount                invoice_date    product_id           qty  \\\n",
      "count  20000.000000                       20000  20000.000000  20000.000000   \n",
      "mean      52.918236  1995-06-11 13:32:18.240000    149.746700      5.005900   \n",
      "min        5.010000         1970-01-05 00:00:00    100.000000      1.000000   \n",
      "25%       29.137500         1982-08-02 00:00:00    125.000000      3.000000   \n",
      "50%       53.485000         1994-12-26 00:00:00    150.000000      5.000000   \n",
      "75%       76.520000         2008-03-01 12:00:00    175.000000      7.000000   \n",
      "max       99.990000         2022-01-17 00:00:00    199.000000      9.000000   \n",
      "std       27.433893                         NaN     28.727468      2.576703   \n",
      "\n",
      "            revenue    stock_code          year         month           day  \\\n",
      "count  20000.000000  2.000000e+04  20000.000000  20000.000000  20000.000000   \n",
      "mean     265.687038  4.950036e+07   1994.941400      6.541900     15.869600   \n",
      "min        5.070000  1.977000e+03   1970.000000      1.000000      1.000000   \n",
      "25%       93.602500  2.425234e+07   1982.000000      4.000000      8.000000   \n",
      "50%      205.940000  4.931714e+07   1994.000000      7.000000     16.000000   \n",
      "75%      391.672500  7.457446e+07   2008.000000     10.000000     24.000000   \n",
      "max      898.920000  9.999216e+07   2022.000000     12.000000     31.000000   \n",
      "std      208.079422  2.903008e+07     14.880042      3.439889      8.867216   \n",
      "\n",
      "          dayofweek  \n",
      "count  20000.000000  \n",
      "mean       3.003200  \n",
      "min        0.000000  \n",
      "25%        1.000000  \n",
      "50%        3.000000  \n",
      "75%        5.000000  \n",
      "max        6.000000  \n",
      "std        2.009724  \n",
      "\n",
      "Correlation matrix (numeric columns):\n",
      "              qty    amount   revenue\n",
      "qty      1.000000  0.011086  0.660933\n",
      "amount   0.011086  1.000000  0.675678\n",
      "revenue  0.660933  0.675678  1.000000\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid frequency: DE, failed to parse with error message: ValueError(\"Invalid frequency: DE, failed to parse with error message: KeyError('DE')\")",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/tslibs/offsets.pyx:4776\u001b[39m, in \u001b[36mpandas._libs.tslibs.offsets._get_offset\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'DE'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/tslibs/offsets.pyx:4946\u001b[39m, in \u001b[36mpandas._libs.tslibs.offsets.to_offset\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/tslibs/offsets.pyx:4782\u001b[39m, in \u001b[36mpandas._libs.tslibs.offsets._get_offset\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m: Invalid frequency: DE, failed to parse with error message: KeyError('DE')",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m     app.run()\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      4\u001b[39m df = preprocess_data(df)\n\u001b[32m      5\u001b[39m explore_data(df)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m app = \u001b[43mcreate_dashboard\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m app.run()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[50]\u001b[39m\u001b[32m, line 57\u001b[39m, in \u001b[36mcreate_dashboard\u001b[39m\u001b[34m(df)\u001b[39m\n\u001b[32m     55\u001b[39m     data_m,model_m,predictions_m = temporal_prediction(dataset,time=\u001b[33m\"\u001b[39m\u001b[33mmonth\u001b[39m\u001b[33m\"\u001b[39m,periods=\u001b[32m12\u001b[39m)\n\u001b[32m     56\u001b[39m     figure_pred_month.append(display_temporal_prediction(data_m,model_m,predictions_m))\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m     data_m,model_m,predictions_m = \u001b[43mtemporal_prediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtime\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mday\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mperiods\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m120\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m     figure_pred_day.append(display_temporal_prediction(data_m,model_m,predictions_m))\n\u001b[32m     59\u001b[39m year_to_index = {\n\u001b[32m     60\u001b[39m \u001b[33m\"\u001b[39m\u001b[33mYA\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m0\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mMA\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m0\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mDA\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m0\u001b[39m,    \u001b[38;5;66;03m# 1900 (All)\u001b[39;00m\n\u001b[32m     61\u001b[39m \u001b[33m\"\u001b[39m\u001b[33mY2020\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m1\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mM2020\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m1\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mD2020\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m1\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     67\u001b[39m \u001b[33m\"\u001b[39m\u001b[33mY1990\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m7\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mM1990\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m7\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mD1990\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m7\u001b[39m,\n\u001b[32m     68\u001b[39m \u001b[33m\"\u001b[39m\u001b[33mY1980\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m8\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mM1980\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m8\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mD1980\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m8\u001b[39m}\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 53\u001b[39m, in \u001b[36mtemporal_prediction\u001b[39m\u001b[34m(df, time, periods)\u001b[39m\n\u001b[32m     50\u001b[39m model = Prophet()\n\u001b[32m     51\u001b[39m model.fit(dataset)\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m future_dates = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmake_future_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mperiods\u001b[49m\u001b[43m=\u001b[49m\u001b[43mperiods\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreq\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m prediction = model.predict(future_dates)\n\u001b[32m     56\u001b[39m new_df = pd.merge(dataset[[\u001b[33m'\u001b[39m\u001b[33mds\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33my\u001b[39m\u001b[33m'\u001b[39m]], prediction[[\u001b[33m'\u001b[39m\u001b[33mds\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33myhat\u001b[39m\u001b[33m'\u001b[39m]], on=\u001b[33m'\u001b[39m\u001b[33mds\u001b[39m\u001b[33m'\u001b[39m, how=\u001b[33m'\u001b[39m\u001b[33mouter\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\leoma\\Documents\\anaconda3\\envs\\prophet_env\\Lib\\site-packages\\prophet\\forecaster.py:1854\u001b[39m, in \u001b[36mProphet.make_future_dataframe\u001b[39m\u001b[34m(self, periods, freq, include_history)\u001b[39m\n\u001b[32m   1852\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mUnable to infer `freq`\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m   1853\u001b[39m last_date = \u001b[38;5;28mself\u001b[39m.history_dates.max()\n\u001b[32m-> \u001b[39m\u001b[32m1854\u001b[39m dates = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdate_range\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1855\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlast_date\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1856\u001b[39m \u001b[43m    \u001b[49m\u001b[43mperiods\u001b[49m\u001b[43m=\u001b[49m\u001b[43mperiods\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# An extra in case we include start\u001b[39;49;00m\n\u001b[32m   1857\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfreq\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1858\u001b[39m dates = dates[dates > last_date]  \u001b[38;5;66;03m# Drop start if equals last_date\u001b[39;00m\n\u001b[32m   1859\u001b[39m dates = dates[:periods]  \u001b[38;5;66;03m# Return correct number of periods\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\leoma\\Documents\\anaconda3\\envs\\prophet_env\\Lib\\site-packages\\pandas\\core\\indexes\\datetimes.py:1008\u001b[39m, in \u001b[36mdate_range\u001b[39m\u001b[34m(start, end, periods, freq, tz, normalize, name, inclusive, unit, **kwargs)\u001b[39m\n\u001b[32m   1005\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m freq \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m com.any_none(periods, start, end):\n\u001b[32m   1006\u001b[39m     freq = \u001b[33m\"\u001b[39m\u001b[33mD\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1008\u001b[39m dtarr = \u001b[43mDatetimeArray\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_generate_range\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1009\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1010\u001b[39m \u001b[43m    \u001b[49m\u001b[43mend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1011\u001b[39m \u001b[43m    \u001b[49m\u001b[43mperiods\u001b[49m\u001b[43m=\u001b[49m\u001b[43mperiods\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1012\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfreq\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfreq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtz\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[43m    \u001b[49m\u001b[43minclusive\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclusive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[43m    \u001b[49m\u001b[43munit\u001b[49m\u001b[43m=\u001b[49m\u001b[43munit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1017\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1018\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1019\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m DatetimeIndex._simple_new(dtarr, name=name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\leoma\\Documents\\anaconda3\\envs\\prophet_env\\Lib\\site-packages\\pandas\\core\\arrays\\datetimes.py:423\u001b[39m, in \u001b[36mDatetimeArray._generate_range\u001b[39m\u001b[34m(cls, start, end, periods, freq, tz, normalize, ambiguous, nonexistent, inclusive, unit)\u001b[39m\n\u001b[32m    418\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m com.count_not_none(start, end, periods, freq) != \u001b[32m3\u001b[39m:\n\u001b[32m    419\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    420\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mOf the four parameters: start, end, periods, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    421\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mand freq, exactly three must be specified\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    422\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m423\u001b[39m freq = \u001b[43mto_offset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    425\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m start \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    426\u001b[39m     start = Timestamp(start)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/tslibs/offsets.pyx:4791\u001b[39m, in \u001b[36mpandas._libs.tslibs.offsets.to_offset\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/tslibs/offsets.pyx:4954\u001b[39m, in \u001b[36mpandas._libs.tslibs.offsets.to_offset\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m: Invalid frequency: DE, failed to parse with error message: ValueError(\"Invalid frequency: DE, failed to parse with error message: KeyError('DE')\")"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    file_path = \"invoices.csv\"\n",
    "    df = load_data(file_path)\n",
    "    df = preprocess_data(df)\n",
    "    explore_data(df)\n",
    "\n",
    "    app = create_dashboard(df)\n",
    "    app.run()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d138a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prophet_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
